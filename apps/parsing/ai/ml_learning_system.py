import logging
import numpy as np
import asyncio
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any
from collections import defaultdict, deque

logger = logging.getLogger('parser.ai.learning')


class MLLearningSystem:
    """üéØ –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –û–ë–£–ß–ï–ù–ò–Ø –î–õ–Ø –í–°–ï–• ML –ú–û–î–ï–õ–ï–ô"""

    def __init__(self, db_path="ml_knowledge.db"):
        self.db_path = db_path

        # üî• –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô
        self.freshness_patterns = {}
        self.price_patterns = {}
        self.timing_optimization = {}
        self.successful_queries = {}
        self.learning_history = deque(maxlen=2000)

        # üî• –î–û–ë–ê–í–¨–¢–ï –≠–¢–ò –ê–¢–†–ò–ë–£–¢–´ –î–õ–Ø –û–ë–†–ê–¢–ù–û–ô –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–ò
        self.successful_patterns = []  # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏
        self.failed_patterns = []  # –î–ª—è –æ—à–∏–±–æ–∫
        self.feedback_history = []  # –ò—Å—Ç–æ—Ä–∏—è —Ñ–∏–¥–±–µ–∫–∞

        logger.info("üß† –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")

    async def learn_from_product(self, product_data: Dict[str, Any]):
        """üéØ –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–∞"""
        try:
            learning_entry = {
                'timestamp': datetime.now().isoformat(),
                'product_data': product_data,
                'freshness_score': product_data.get('ml_freshness_score', 0),
                'predicted_price': product_data.get('ai_predicted_price', 0),
                'actual_price': product_data.get('price', 0),
                'category': product_data.get('category', 'unknown')
            }

            self.learning_history.append(learning_entry)

            # üî• –û–ë–£–ß–ê–ï–ú –†–ê–ó–ù–´–ï –ê–°–ü–ï–ö–¢–´
            category = product_data.get('category', 'unknown')
            found_time = datetime.now()

            await self._update_timing_patterns(category, found_time)
            await self._update_freshness_patterns(product_data, category)
            await self._update_price_patterns(product_data, category)

            # üî• –û–ë–£–ß–ê–ï–ú–°–Ø –ù–ê –£–°–ü–ï–®–ù–´–• –ó–ê–ü–†–û–°–ê–•
            query = product_data.get('search_query', '')
            if query:
                await self._update_successful_queries(query, category)

            logger.debug(f"üìö –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç–æ–≤–∞—Ä–µ: {product_data.get('name', 'Unknown')}")
            return True

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —Ç–æ–≤–∞—Ä–µ: {e}")
            return False

    async def _update_timing_patterns(self, category, found_time):
        """üïí –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        try:
            if isinstance(found_time, str):
                found_time = datetime.fromisoformat(found_time.replace('Z', '+00:00'))

            hour = found_time.hour
            day_of_week = found_time.weekday()

            if category not in self.timing_optimization:
                self.timing_optimization[category] = {
                    'hourly_pattern': [0] * 24,
                    'daily_pattern': [0] * 7,
                    'total_finds': 0
                }

            self.timing_optimization[category]['hourly_pattern'][hour] += 1
            self.timing_optimization[category]['daily_pattern'][day_of_week] += 1
            self.timing_optimization[category]['total_finds'] += 1

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è timing patterns: {e}")

    async def _update_freshness_patterns(self, product_data, category):
        """üî• –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Å–≤–µ–∂–µ—Å—Ç–∏"""
        try:
            if category not in self.freshness_patterns:
                self.freshness_patterns[category] = {
                    'feature_counts': defaultdict(int),
                    'total_samples': 0
                }

            # –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–≤–µ–∂–µ—Å—Ç–∏
            features = self._extract_freshness_features(product_data)
            for feature, value in features.items():
                if value > 0.5:
                    feature_key = f"{feature}_{value:.1f}"
                    self.freshness_patterns[category]['feature_counts'][feature_key] += 1

            self.freshness_patterns[category]['total_samples'] += 1

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è freshness patterns: {e}")

    async def _update_price_patterns(self, product_data, category):
        """üí∞ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Ü–µ–Ω"""
        try:
            if category not in self.price_patterns:
                self.price_patterns[category] = {
                    'price_ranges': defaultdict(int),
                    'total_samples': 0,
                    'avg_price': 0
                }

            price = product_data.get('price', 0)
            if price > 0:
                price_range = f"{int(price / 1000) * 1000}-{int(price / 1000) * 1000 + 1000}"
                self.price_patterns[category]['price_ranges'][price_range] += 1
                self.price_patterns[category]['total_samples'] += 1

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è price patterns: {e}")

    async def _update_successful_queries(self, query, category):
        """üîç –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
        try:
            if category not in self.successful_queries:
                self.successful_queries[category] = {}

            if query not in self.successful_queries[category]:
                self.successful_queries[category][query] = 0

            self.successful_queries[category][query] += 1

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è successful queries: {e}")

    def _extract_freshness_features(self, product_data):
        """üîç –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–≤–µ–∂–µ—Å—Ç–∏"""
        try:
            title = product_data.get('name', '').lower()
            description = product_data.get('description', '').lower()
            text = f"{title} {description}"

            features = {}

            # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å–≤–µ–∂–µ—Å—Ç–∏
            freshness_keywords = ['—Ç–æ–ª—å–∫–æ —á—Ç–æ', '—Å–µ–≥–æ–¥–Ω—è', '–º–∏–Ω—É—Ç', '—á–∞—Å', '—Ç–æ–ª—å–∫–æ –¥–æ–±–∞–≤–ª–µ–Ω', '—Å—Ä–æ—á–Ω–æ']
            for keyword in freshness_keywords:
                features[f'keyword_{keyword}'] = 1.0 if keyword in text else 0.0

            # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            time_listed = product_data.get('time_listed', 24)
            features['time_listed'] = min(time_listed / 24.0, 1.0)
            features['is_today'] = 1.0 if '—Å–µ–≥–æ–¥–Ω—è' in str(product_data.get('posted_date', '')).lower() else 0.0

            # –ü—Ä–∏–∑–Ω–∞–∫–∏ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
            features['has_images'] = 1.0 if product_data.get('images') else 0.0
            features['seller_rating'] = min(product_data.get('seller_rating', 0) / 5.0, 1.0)

            return features

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–≤–µ–∂–µ—Å—Ç–∏: {e}")
            return {}

    async def get_optimal_search_times(self, category):
        """üïí –õ—É—á—à–µ–µ –≤—Ä–µ–º—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        try:
            if category in self.timing_optimization:
                pattern = self.timing_optimization[category]['hourly_pattern']
                total_finds = self.timing_optimization[category]['total_finds']

                if total_finds > 10:
                    normalized_pattern = [count / total_finds for count in pattern]
                    best_hours = sorted(range(24), key=lambda h: normalized_pattern[h], reverse=True)[:3]
                    return best_hours

            return [9, 14, 19]  # –£—Ç—Ä–æ, –¥–µ–Ω—å, –≤–µ—á–µ—Ä

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏: {e}")
            return [9, 14, 19]

    async def get_successful_queries(self, category, limit=5):
        """üîç –°–∞–º—ã–µ —É—Å–ø–µ—à–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        try:
            if category in self.successful_queries:
                queries = self.successful_queries[category]
                sorted_queries = sorted(queries.items(), key=lambda x: x[1], reverse=True)
                return [q[0] for q in sorted_queries[:limit]]
            return []

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {e}")
            return []

    async def get_learning_insights(self, category):
        """üìä –ò–Ω—Å–∞–π—Ç—ã –ø–æ –æ–±—É—á–µ–Ω–∏—é –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        try:
            insights = {
                'optimal_times': await self.get_optimal_search_times(category),
                'successful_queries': await self.get_successful_queries(category),
                'total_learned_samples': len([x for x in self.learning_history if x.get('category') == category]),
                'freshness_patterns_count': self.freshness_patterns.get(category, {}).get('total_samples', 0),
                'price_patterns_count': self.price_patterns.get(category, {}).get('total_samples', 0),
                'confidence_level': 'medium'
            }

            total_samples = insights['total_learned_samples']
            if total_samples > 100:
                insights['confidence_level'] = 'high'
            elif total_samples > 20:
                insights['confidence_level'] = 'medium'
            else:
                insights['confidence_level'] = 'low'

            return insights

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Å–∞–π—Ç–æ–≤: {e}")
            return {}

    async def get_learning_stats(self):
        """üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è"""
        try:
            total_categories = len(self.timing_optimization)
            total_samples = len(self.learning_history)

            intelligence_level = min(total_samples / 200.0, 1.0)

            return {
                'total_categories_learned': total_categories,
                'total_samples_collected': total_samples,
                'intelligence_level': f"{intelligence_level:.1%}",
                'system_confidence': 'high' if intelligence_level > 0.7 else 'medium' if intelligence_level > 0.3 else 'low',
                'last_learning_update': datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è: {e}")
            return {}

    async def save_learning_state(self):
        """üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è"""
        try:
            state = {
                'freshness_patterns': dict(self.freshness_patterns),
                'price_patterns': dict(self.price_patterns),
                'timing_optimization': dict(self.timing_optimization),
                'successful_queries': dict(self.successful_queries),
                'learning_history': list(self.learning_history),
                'last_saved': datetime.now().isoformat()
            }

            with open('ml_learning_state.json', 'w', encoding='utf-8') as f:
                json.dump(state, f, ensure_ascii=False, indent=2)

            logger.info("üíæ –°–æ—Å—Ç–æ—è–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è: {e}")

    async def collect_feedback(self, prediction, actual_result, features, prediction_type, confidence, context):
        """üì• –°–±–æ—Ä –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
        try:
            feedback_data = {
                'timestamp': datetime.now().isoformat(),
                'prediction_type': prediction_type,
                'prediction': prediction,
                'actual_result': actual_result,
                'features': features,
                'confidence': confidence,
                'context': context,
                'error': abs(prediction - actual_result) if actual_result is not None else 0
            }

            # üî• –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –¢–µ–ø–µ—Ä—å –∞—Ç—Ä–∏–±—É—Ç —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            self.successful_patterns.append(feedback_data)
            self.feedback_history.append(feedback_data)

            logger.debug(f"üì• –°–æ–±—Ä–∞–Ω–∞ –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –¥–ª—è {prediction_type}")

            return True
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏: {e}")
            return False

    async def load_learning_state(self):
        """üì• –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è"""
        try:
            with open('ml_learning_state.json', 'r', encoding='utf-8') as f:
                state = json.load(f)

            self.freshness_patterns = state.get('freshness_patterns', {})
            self.price_patterns = state.get('price_patterns', {})
            self.timing_optimization = state.get('timing_optimization', {})
            self.successful_queries = state.get('successful_queries', {})
            self.learning_history = deque(state.get('learning_history', []), maxlen=2000)

            logger.info("üì• –°–æ—Å—Ç–æ—è–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–æ")
            return True

        except FileNotFoundError:
            logger.info("üì• –§–∞–π–ª —Å–æ—Å—Ç–æ—è–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω, –Ω–∞—á–∏–Ω–∞–µ–º —Å —á–∏—Å—Ç–æ–≥–æ –ª–∏—Å—Ç–∞")
            return True
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è: {e}")
            return False