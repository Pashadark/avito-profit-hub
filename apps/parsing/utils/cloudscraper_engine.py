# üìÅ apps/parsing/utils/cloudscraper_engine.py
import cloudscraper
import logging
import re
import time
import random
import os
from typing import Optional, Dict, Any, List, Tuple
from urllib.parse import quote_plus
from bs4 import BeautifulSoup
import requests

logger = logging.getLogger('parser.cloudscraper')


# üî• –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –ò–ó .env
def load_proxy_data_from_env():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ–∫—Å–∏ –∏–∑ .env —Ñ–∞–π–ª–∞"""
    return {
        'MOBILE_PROXY_HTTP': os.getenv('MOBILE_PROXY_HTTP'),
        'MOBILE_PROXY_SOCKS5': os.getenv('MOBILE_PROXY_SOCKS5'),
        'PROXYMARKET_USER1': os.getenv('PROXYMARKET_USER1'),
        'PROXYMARKET_PASS1': os.getenv('PROXYMARKET_PASS1'),
        'PROXYMARKET_USER2': os.getenv('PROXYMARKET_USER2'),
        'PROXYMARKET_PASS2': os.getenv('PROXYMARKET_PASS2'),
        'PROXY_CHANGE_LOGIN': os.getenv('PROXY_CHANGE_LOGIN'),
        'PROXY_CHANGE_PASSWORD': os.getenv('PROXY_CHANGE_PASSWORD'),
    }


class ProxyManager:
    """–£–ú–ù–´–ô –ú–ï–ù–ï–î–ñ–ï–† –ü–†–û–ö–°–ò –° –ê–í–¢–û–†–û–¢–ê–¶–ò–ï–ô"""

    def __init__(self):
        # üî• –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –ò–ó .env
        proxy_data = load_proxy_data_from_env()

        # üî• –í–°–ï –ù–ê–®–ò –ü–†–û–ö–°–ò (–±–µ—Ä—ë–º –∏–∑ .env + –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∞)
        self.proxy_pool = self._build_proxy_pool(proxy_data)

        self.current_proxy_index = 0
        self.max_fails_before_block = int(os.getenv('PARSER_MAX_RETRIES', 3))
        self.proxy_timeout = int(os.getenv('PARSER_TIMEOUT', 20))

        logger.info(f"‚úÖ ProxyManager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {len(self.proxy_pool)} –ø—Ä–æ–∫—Å–∏")
        self._log_proxy_status()

    def _build_proxy_pool(self, proxy_data):
        """–°—Ç—Ä–æ–∏—Ç –ø—É–ª –ø—Ä–æ–∫—Å–∏ –∏–∑ –¥–∞–Ω–Ω—ã—Ö .env + –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö"""
        proxy_pool = []

        # üî• –ü–†–ï–ú–ò–£–ú –ü–†–û–ö–°–ò –ò–ó .env (–µ—Å–ª–∏ –µ—Å—Ç—å)
        # –ú–æ–±–∏–ª—å–Ω—ã–µ –ø—Ä–æ–∫—Å–∏ –ú–æ—Å–∫–≤–∞ (–ú–µ–≥–∞—Ñ–æ–Ω)
        mobile_http = proxy_data.get('MOBILE_PROXY_HTTP')
        if mobile_http:
            proxy_pool.append({
                'name': 'Mobile Moscow Megafon',
                'url': mobile_http,
                'type': 'premium_mobile',
                'geo': '–ú–æ—Å–∫–≤–∞',
                'operator': 'Megafon',
                'priority': 10,  # –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                'last_used': 0,
                'success_count': 0,
                'fail_count': 0,
                'blocked': False
            })

        # SOCKS5 –≤–µ—Ä—Å–∏—è
        mobile_socks5 = proxy_data.get('MOBILE_PROXY_SOCKS5')
        if mobile_socks5:
            proxy_pool.append({
                'name': 'Mobile Moscow SOCKS5',
                'url': mobile_socks5,
                'type': 'premium_socks5',
                'geo': '–ú–æ—Å–∫–≤–∞',
                'operator': 'Megafon',
                'priority': 9,
                'last_used': 0,
                'success_count': 0,
                'fail_count': 0,
                'blocked': False
            })

        # üî• PROXY.MARKET –ü–£–õ - 10000-10999 –ü–û–†–¢–û–í!
        proxymarket_user1 = proxy_data.get('PROXYMARKET_USER1')
        proxymarket_pass1 = proxy_data.get('PROXYMARKET_PASS1')
        proxymarket_user2 = proxy_data.get('PROXYMARKET_USER2')
        proxymarket_pass2 = proxy_data.get('PROXYMARKET_PASS2')

        # –ü–ï–†–í–´–ô –ê–ö–ö–ê–£–ù–¢ (Bf5Bok2pTLp7) - –ø–µ—Ä–≤—ã–µ 50 –ø–æ—Ä—Ç–æ–≤
        if proxymarket_user1 and proxymarket_pass1:
            for port in range(10000, 10050):
                proxy_pool.append({
                    'name': f'ProxyMarket User1:{port}',
                    'url': f'http://{proxymarket_user1}:{proxymarket_pass1}@pool.proxy.market:{port}',
                    'type': 'proxy_market_pool',
                    'geo': 'RU',
                    'operator': 'ProxyMarket',
                    'priority': 8,
                    'last_used': 0,
                    'success_count': 0,
                    'fail_count': 0,
                    'blocked': False
                })

        # –í–¢–û–†–û–ô –ê–ö–ö–ê–£–ù–¢ (eIqYgUe7Yybs) - —Å–ª–µ–¥—É—é—â–∏–µ 50 –ø–æ—Ä—Ç–æ–≤
        if proxymarket_user2 and proxymarket_pass2:
            for port in range(10050, 10100):
                proxy_pool.append({
                    'name': f'ProxyMarket User2:{port}',
                    'url': f'http://{proxymarket_user2}:{proxymarket_pass2}@pool.proxy.market:{port}',
                    'type': 'proxy_market_pool',
                    'geo': 'RU',
                    'operator': 'ProxyMarket',
                    'priority': 7,
                    'last_used': 0,
                    'success_count': 0,
                    'fail_count': 0,
                    'blocked': False
                })

        # üî• –ë–ï–°–ü–õ–ê–¢–ù–´–ï –ü–†–û–ö–°–ò –î–õ–Ø –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø
        proxy_pool.extend(self._get_free_proxies_for_testing())

        # üî• –†–ï–ó–ï–†–í–ù–´–ô: –±–µ–∑ –ø—Ä–æ–∫—Å–∏
        proxy_pool.append({
            'name': 'DIRECT (–±–µ–∑ –ø—Ä–æ–∫—Å–∏)',
            'url': None,
            'type': 'direct',
            'geo': '–ü—Ä—è–º–æ–µ',
            'operator': 'Local IP',
            'priority': 1,
            'last_used': 0,
            'success_count': 0,
            'fail_count': 0,
            'blocked': False
        })

        return proxy_pool

    def _get_free_proxies_for_testing(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        return [
            {
                'name': 'FreeProxy #1 (US)',
                'url': 'http://45.77.56.109:3128',
                'type': 'free_http',
                'geo': 'USA',
                'operator': 'FreeProxy',
                'priority': 6,
                'last_used': 0,
                'success_count': 0,
                'fail_count': 0,
                'blocked': False
            },
            {
                'name': 'FreeProxy #2 (SG)',
                'url': 'http://103.152.112.145:80',
                'type': 'free_http',
                'geo': 'Singapore',
                'operator': 'FreeProxy',
                'priority': 5,
                'last_used': 0,
                'success_count': 0,
                'fail_count': 0,
                'blocked': False
            },
            {
                'name': 'FreeProxy #3 (DE)',
                'url': 'http://194.169.167.5:8080',
                'type': 'free_http',
                'geo': 'Germany',
                'operator': 'FreeProxy',
                'priority': 5,
                'last_used': 0,
                'success_count': 0,
                'fail_count': 0,
                'blocked': False
            },
            {
                'name': 'FreeProxy #4 (RU)',
                'url': 'http://193.56.255.230:3128',
                'type': 'free_http',
                'geo': 'Russia',
                'operator': 'FreeProxy',
                'priority': 5,
                'last_used': 0,
                'success_count': 0,
                'fail_count': 0,
                'blocked': False
            },
            {
                'name': 'FreeProxy #5 (NL)',
                'url': 'http://185.162.231.189:80',
                'type': 'free_http',
                'geo': 'Netherlands',
                'operator': 'FreeProxy',
                'priority': 5,
                'last_used': 0,
                'success_count': 0,
                'fail_count': 0,
                'blocked': False
            },
            {
                'name': 'FreeProxy #6 (FR)',
                'url': 'http://51.159.115.233:3128',
                'type': 'free_http',
                'geo': 'France',
                'operator': 'FreeProxy',
                'priority': 4,
                'last_used': 0,
                'success_count': 0,
                'fail_count': 0,
                'blocked': False
            },
            {
                'name': 'FreeProxy #7 (UK)',
                'url': 'http://51.89.15.86:8080',
                'type': 'free_http',
                'geo': 'UK',
                'operator': 'FreeProxy',
                'priority': 4,
                'last_used': 0,
                'success_count': 0,
                'fail_count': 0,
                'blocked': False
            },
            {
                'name': 'FreeProxy #8 (CA)',
                'url': 'http://142.93.113.142:3128',
                'type': 'free_http',
                'geo': 'Canada',
                'operator': 'FreeProxy',
                'priority': 4,
                'last_used': 0,
                'success_count': 0,
                'fail_count': 0,
                'blocked': False
            },
            {
                'name': 'FreeProxy #9 (JP)',
                'url': 'http://133.18.201.195:8080',
                'type': 'free_http',
                'geo': 'Japan',
                'operator': 'FreeProxy',
                'priority': 4,
                'last_used': 0,
                'success_count': 0,
                'fail_count': 0,
                'blocked': False
            },
            {
                'name': 'FreeProxy #10 (IN)',
                'url': 'http://103.216.82.18:6666',
                'type': 'free_http',
                'geo': 'India',
                'operator': 'FreeProxy',
                'priority': 3,
                'last_used': 0,
                'success_count': 0,
                'fail_count': 0,
                'blocked': False
            }
        ]

    def _log_proxy_status(self):
        """–õ–æ–≥–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç—É—Å –≤—Å–µ—Ö –ø—Ä–æ–∫—Å–∏"""
        logger.info(
            f"üìä –°–¢–ê–¢–£–° –ü–†–û–ö–°–ò: {len([p for p in self.proxy_pool if not p['blocked']])}/{len(self.proxy_pool)} –∞–∫—Ç–∏–≤–Ω—ã")

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø—É –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏
        type_stats = {}
        for proxy in self.proxy_pool:
            proxy_type = proxy['type']
            if proxy_type not in type_stats:
                type_stats[proxy_type] = {'total': 0, 'active': 0}
            type_stats[proxy_type]['total'] += 1
            if not proxy['blocked']:
                type_stats[proxy_type]['active'] += 1

        for proxy_type, stats in type_stats.items():
            logger.info(f"  {proxy_type}: {stats['active']}/{stats['total']} –∞–∫—Ç–∏–≤–Ω—ã")

    def get_next_proxy(self) -> Optional[Dict]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–π —Ä–∞–±–æ—á–∏–π –ø—Ä–æ–∫—Å–∏"""
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É –∏ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        available_proxies = [
            p for p in self.proxy_pool
            if not p['blocked']
        ]

        if not available_proxies:
            logger.error("‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏, –≤—Å–µ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω—ã!")
            return None

        # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–æ–∫—Å–∏ —Å –Ω–∞–∏–º–µ–Ω—å—à–∏–º –≤—Ä–µ–º–µ–Ω–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏ –≤—ã—Å–æ–∫–∏–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º
        available_proxies.sort(key=lambda x: (x['last_used'], -x['priority']))
        proxy = available_proxies[0]

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        proxy['last_used'] = time.time()
        self.current_proxy_index = self.proxy_pool.index(proxy)

        logger.info(f"üîÑ –í—ã–±—Ä–∞–Ω –ø—Ä–æ–∫—Å–∏: {proxy['name']} ({proxy['geo']})")

        if proxy['url']:
            return {
                'http': proxy['url'],
                'https': proxy['url']
            }
        else:
            return None  # –ü—Ä—è–º–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ

    def mark_success(self, items_found: int = 0):
        """–û—Ç–º–µ—á–∞–µ–º —É—Å–ø–µ—à–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∫—Å–∏"""
        proxy = self.proxy_pool[self.current_proxy_index]
        proxy['success_count'] += 1

        if items_found > 0:
            proxy['priority'] = min(10, proxy['priority'] + 1)  # –ü–æ–≤—ã—à–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
            logger.info(f"‚úÖ –ü—Ä–æ–∫—Å–∏ {proxy['name']} —É—Å–ø–µ—à–µ–Ω (—Ç–æ–≤–∞—Ä–æ–≤: {items_found}), –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {proxy['priority']}")
        else:
            logger.info(f"‚ö†Ô∏è –ü—Ä–æ–∫—Å–∏ {proxy['name']} –≤–µ—Ä–Ω—É–ª 0 —Ç–æ–≤–∞—Ä–æ–≤")

    def mark_failed(self, reason: str = "unknown"):
        """–û—Ç–º–µ—á–∞–µ–º –Ω–µ—É–¥–∞—á–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∫—Å–∏"""
        proxy = self.proxy_pool[self.current_proxy_index]
        proxy['fail_count'] += 1

        logger.warning(f"‚ùå –ü—Ä–æ–∫—Å–∏ {proxy['name']} –Ω–µ —É–¥–∞–ª—Å—è: {reason}")

        # –ï—Å–ª–∏ –º–Ω–æ–≥–æ —Ñ–µ–π–ª–æ–≤ - –±–ª–æ–∫–∏—Ä—É–µ–º
        if proxy['fail_count'] >= self.max_fails_before_block:
            proxy['blocked'] = True
            proxy['priority'] = 0
            logger.error(f"üö´ –ü—Ä–æ–∫—Å–∏ {proxy['name']} –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω –ø–æ—Å–ª–µ {proxy['fail_count']} —Ñ–µ–π–ª–æ–≤")

    def mark_blocked_by_avito(self):
        """–û—Ç–º–µ—á–∞–µ–º —á—Ç–æ Avito –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª —ç—Ç–æ—Ç –ø—Ä–æ–∫—Å–∏"""
        proxy = self.proxy_pool[self.current_proxy_index]
        proxy['blocked'] = True
        proxy['priority'] = 0

        logger.error(f"üö´ Avito –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –ø—Ä–æ–∫—Å–∏: {proxy['name']}")

    def rotate_ip_for_mobile_proxy(self):
        """–ú–µ–Ω—è–µ—Ç IP –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏ (–µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è)"""
        try:
            proxy_data = load_proxy_data_from_env()
            login = proxy_data.get('PROXY_CHANGE_LOGIN')
            password = proxy_data.get('PROXY_CHANGE_PASSWORD')

            if not login or not password:
                logger.warning("‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –¥–ª—è —Å–º–µ–Ω—ã IP –Ω–µ —É–∫–∞–∑–∞–Ω—ã –≤ .env")
                return False

            # –î–ª—è mobileproxy.space
            change_url = "https://mobileproxy.space/api/v1/change_ip"
            params = {'login': login, 'password': password}

            response = requests.get(change_url, params=params, timeout=10)

            if response.status_code == 200:
                data = response.json()
                new_ip = data.get('new_ip', '–Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω')
                logger.info(f"üîÑ IP –∏–∑–º–µ–Ω–µ–Ω –¥–ª—è Mobile Moscow: {new_ip}")

                # –†–∞–∑–±–ª–æ–∫–∏—Ä—É–µ–º –ø—Ä–æ–∫—Å–∏ –ø–æ—Å–ª–µ —Å–º–µ–Ω—ã IP
                for proxy in self.proxy_pool:
                    if 'mobile' in proxy['type']:
                        proxy['blocked'] = False
                        proxy['fail_count'] = 0

                return True
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–º–µ–Ω–∏—Ç—å IP: {e}")

        return False

    def get_status_report(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç—á–µ—Ç –æ —Å—Ç–∞—Ç—É—Å–µ –ø—Ä–æ–∫—Å–∏"""
        total = len(self.proxy_pool)
        active = len([p for p in self.proxy_pool if not p['blocked']])
        blocked = total - active

        report = [
            f"üìä –û–¢–ß–ï–¢ –ü–†–û–ö–°–ò: {active}/{total} –∞–∫—Ç–∏–≤–Ω—ã",
            f"üü¢ –ê–∫—Ç–∏–≤–Ω—ã–µ: {active} | üî¥ –ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ: {blocked}"
        ]

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø—É
        type_stats = {}
        for proxy in self.proxy_pool:
            proxy_type = proxy['type']
            if proxy_type not in type_stats:
                type_stats[proxy_type] = {'total': 0, 'active': 0}
            type_stats[proxy_type]['total'] += 1
            if not proxy['blocked']:
                type_stats[proxy_type]['active'] += 1

        for proxy_type, stats in type_stats.items():
            report.append(f"  {proxy_type}: {stats['active']}/{stats['total']} –∞–∫—Ç–∏–≤–Ω—ã")

        return "\n".join(report)


class CloudscraperEngine:
    """–£–õ–£–ß–®–ï–ù–ù–´–ô CLOUDSCRAPER –° –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ô –†–û–¢–ê–¶–ò–ï–ô –ü–†–û–ö–°–ò"""

    def __init__(self, user_agent: Optional[str] = None, city: str = "–ú–æ—Å–∫–≤–∞"):
        try:
            logger.info(f"üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è CloudscraperEngine –¥–ª—è {city}")

            # üî• –£–ú–ù–´–ô –ú–ï–ù–ï–î–ñ–ï–† –ü–†–û–ö–°–ò
            self.proxy_manager = ProxyManager()

            # üî• –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô CLOUDSCRAPER
            self.scraper = cloudscraper.create_scraper(
                browser={
                    'browser': 'chrome',
                    'platform': 'windows',
                    'mobile': False
                },
                delay=random.uniform(4, 7),
                interpreter='js2py',
                debug=False
            )

            # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –∫–∞–∫ —É —Ä–µ–∞–ª—å–Ω–æ–≥–æ –±—Ä–∞—É–∑–µ—Ä–∞
            self.headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Cache-Control': 'max-age=0',
                'DNT': '1'
            }

            self.city = city
            self.max_retries = int(os.getenv('PARSER_MAX_RETRIES', 3))
            self.request_timeout = int(os.getenv('PARSER_TIMEOUT', 25))

            logger.info(
                f"‚úÖ CloudscraperEngine –≥–æ—Ç–æ–≤. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–æ—Ç–∞—Ü–∏—è {len(self.proxy_manager.proxy_pool)} –ø—Ä–æ–∫—Å–∏")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ CloudscraperEngine: {e}")
            raise

    def fetch_page_with_retry(self, url: str) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Ä–æ—Ç–∞—Ü–∏–µ–π –ø—Ä–æ–∫—Å–∏ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö"""
        retry_count = 0

        while retry_count < self.max_retries:
            try:
                # –ü–æ–ª—É—á–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π –ø—Ä–æ–∫—Å–∏
                proxies = self.proxy_manager.get_next_proxy()
                proxy_name = self.proxy_manager.proxy_pool[
                    self.proxy_manager.current_proxy_index
                ]['name']

                if proxies is None and retry_count == 0:
                    logger.info("üåê –ü—Ä—è–º–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ (–±–µ–∑ –ø—Ä–æ–∫—Å–∏)")
                elif proxies:
                    logger.info(f"üîó –ò—Å–ø–æ–ª—å–∑—É—é –ø—Ä–æ–∫—Å–∏: {proxy_name}")

                # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ—Ñ–µ—Ä–µ—Ä
                headers = self.headers.copy()
                headers['Referer'] = 'https://www.avito.ru/'

                # –°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º
                time.sleep(random.uniform(0.5, 2.0))

                start_time = time.time()

                # –î–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å
                response = self.scraper.get(
                    url,
                    headers=headers,
                    proxies=proxies,
                    timeout=self.request_timeout
                )

                elapsed = time.time() - start_time

                result = {
                    'html': response.text,
                    'status_code': response.status_code,
                    'url': response.url,
                    'elapsed_time': elapsed,
                    'engine': 'cloudscraper',
                    'proxy_used': proxy_name,
                    'proxied': proxies is not None
                }

                # –ê–Ω–∞–ª–∏–∑ –æ—Ç–≤–µ—Ç–∞
                if self._is_blocked(response.text):
                    result['blocked'] = True
                    result['blocked_reason'] = self._detect_block_reason(response.text)

                    logger.warning(
                        f"üö´ –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ —á–µ—Ä–µ–∑ {proxy_name}: {result['blocked_reason']} "
                        f"({elapsed:.2f}—Å, —Å—Ç–∞—Ç—É—Å: {response.status_code})"
                    )

                    # –ü–æ–º–µ—á–∞–µ–º –ø—Ä–æ–∫—Å–∏ –∫–∞–∫ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–π
                    self.proxy_manager.mark_blocked_by_avito()

                    # –ü—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π –ø—Ä–æ–∫—Å–∏
                    retry_count += 1
                    continue

                # –£—Å–ø–µ—Ö!
                result['blocked'] = False

                # –°—á–∏—Ç–∞–µ–º —Ç–æ–≤–∞—Ä—ã
                items_count = response.text.count('data-marker="item"')
                result['items_count'] = items_count

                if items_count > 0:
                    logger.info(
                        f"‚úÖ –£—Å–ø–µ—Ö —á–µ—Ä–µ–∑ {proxy_name}: {items_count} —Ç–æ–≤–∞—Ä–æ–≤ "
                        f"({elapsed:.2f}—Å)"
                    )
                    self.proxy_manager.mark_success(items_count)
                else:
                    logger.warning(
                        f"‚ö†Ô∏è {proxy_name}: 0 —Ç–æ–≤–∞—Ä–æ–≤ ({elapsed:.2f}—Å, —Å—Ç–∞—Ç—É—Å: {response.status_code})"
                    )
                    self.proxy_manager.mark_success(0)

                return result

            except Exception as e:
                logger.error(f"üí• –û—à–∏–±–∫–∞ —á–µ—Ä–µ–∑ {proxy_name}: {e}")
                self.proxy_manager.mark_failed(str(e))
                retry_count += 1
                time.sleep(2)  # –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π

        # –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã
        logger.error(f"‚ùå –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ ({self.max_retries}) –∏—Å—á–µ—Ä–ø–∞–Ω—ã –¥–ª—è {url}")

        # –ü—Ä–æ–±—É–µ–º —Å–º–µ–Ω–∏—Ç—å IP –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏
        logger.info("üîÑ –ü—Ä–æ–±—É—é —Å–º–µ–Ω–∏—Ç—å IP –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏...")
        if self.proxy_manager.rotate_ip_for_mobile_proxy():
            logger.info("üîÑ IP –∏–∑–º–µ–Ω–µ–Ω, –ø—Ä–æ–±—É—é –µ—â–µ —Ä–∞–∑...")
            # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ –ø–æ—Å–ª–µ —Å–º–µ–Ω—è IP
            try:
                proxies = self.proxy_manager.get_next_proxy()
                response = self.scraper.get(url, headers=self.headers,
                                            proxies=proxies, timeout=self.request_timeout)

                if response.status_code == 200 and not self._is_blocked(response.text):
                    items = response.text.count('data-marker="item"')
                    logger.info(f"üéâ –£—Å–ø–µ—Ö –ø–æ—Å–ª–µ —Å–º–µ–Ω—ã IP: {items} —Ç–æ–≤–∞—Ä–æ–≤")
                    return {
                        'html': response.text,
                        'status_code': 200,
                        'items_count': items,
                        'proxy_used': 'MOBILE (–Ω–æ–≤—ã–π IP)',
                        'proxied': True
                    }
            except:
                pass

        return None

    def _is_blocked(self, html: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –±–ª–æ–∫–∏—Ä–æ–≤–∫—É Avito"""
        if not html or len(html) < 100:
            return True

        html_lower = html.lower()

        # –Ø–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
        blocked_indicators = [
            '–¥–æ—Å—Ç—É–ø –æ–≥—Ä–∞–Ω–∏—á–µ–Ω',
            '–ø—Ä–æ–±–ª–µ–º—ã —Å ip',
            'qrator',
            '403 forbidden',
            'captcha',
            '–ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ —á—Ç–æ –≤—ã –Ω–µ —Ä–æ–±–æ—Ç',
            'checking your browser',
            '—Å–µ—Ç—å tor',
            '–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã'
        ]

        # –ü—Ä–∏–∑–Ω–∞–∫–∏ —É—Å–ø–µ—à–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã Avito
        success_indicators = [
            'data-marker="item"',
            'iva-item-root',
            'avito.ru/items/',
            '–æ–±—ä—è–≤–ª–µ–Ω–∏—è'
        ]

        is_blocked = any(indicator in html_lower for indicator in blocked_indicators)
        has_content = any(indicator in html for indicator in success_indicators)

        # –ï—Å–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∞—è –∏–ª–∏ –Ω–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç–∞ Avito
        if len(html) < 50000 and 'avito' not in html_lower:
            return True

        return is_blocked or not has_content

    def _detect_block_reason(self, html: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø—Ä–∏—á–∏–Ω—É –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏"""
        html_lower = html.lower()

        if 'qrator' in html_lower:
            return 'QRATOR (–ø—Ä–æ–∫—Å–∏ –¥–µ—Ç–µ–∫—Ç)'
        elif 'captcha' in html_lower:
            return 'CAPTCHA'
        elif '–¥–æ—Å—Ç—É–ø –æ–≥—Ä–∞–Ω–∏—á–µ–Ω' in html_lower:
            return 'Avito –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞'
        elif 'checking your browser' in html_lower:
            return 'Cloudflare'
        elif '403' in html_lower:
            return '403 Forbidden'
        elif 'tor' in html_lower:
            return 'TOR —Å–µ—Ç—å'
        else:
            return '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞'

    def search_items_fast(self, query: str, max_pages: int = 2, **kwargs) -> Optional[Dict]:
        """–ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Ä–æ—Ç–∞—Ü–∏–µ–π –ø—Ä–æ–∫—Å–∏"""
        max_pages = min(max_pages, int(os.getenv('PARSER_MAX_PAGES', 2)))

        logger.info(f"üîç –ü–æ–∏—Å–∫ '{query}' —Å —Ä–æ—Ç–∞—Ü–∏–µ–π –ø—Ä–æ–∫—Å–∏...")

        all_items = []
        successful_pages = 0

        for page in range(1, max_pages + 1):
            url = self.build_search_url(query, page=page, **kwargs)
            logger.info(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}/{max_pages}")

            result = self.fetch_page_with_retry(url)

            if result is None:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {page}")
                break

            if result.get('blocked'):
                logger.warning(f"üö´ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page} –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–∞: {result.get('blocked_reason')}")
                break

            if not result.get('html'):
                logger.warning(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π HTML –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}")
                continue

            # –ü–∞—Ä—Å–∏–º —Ç–æ–≤–∞—Ä—ã
            items = self._parse_html_advanced(result['html'], query)
            if items:
                all_items.extend(items)
                successful_pages += 1
                logger.info(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(items)} (—á–µ—Ä–µ–∑ {result.get('proxy_used', '?')})")

            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏
            time.sleep(random.uniform(1.5, 3.0))

        # –û—Ç—á–µ—Ç –æ –ø—Ä–æ–∫—Å–∏
        logger.info("\n" + self.proxy_manager.get_status_report())

        return {
            'items': all_items,
            'total_pages': max_pages,
            'successful_pages': successful_pages,
            'engine': 'cloudscraper',
            'proxied': True,
            'success': len(all_items) > 0,
            'total_items': len(all_items),
            'proxy_report': self.proxy_manager.get_status_report()
        }

    def build_search_url(self, query: str, page: int = 1, **kwargs) -> str:
        """–°—Ç—Ä–æ–∏—Ç URL –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        city_map = {
            '–º–æ—Å–∫–≤–∞': 'moskva',
            '—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥': 'sankt-peterburg',
            '–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫': 'novosibirsk',
            '–µ–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥': 'ekaterinburg',
            '–∫–∞–∑–∞–Ω—å': 'kazan'
        }

        city_part = city_map.get(self.city.lower(), 'moskva')
        encoded_query = quote_plus(query)

        url = f"https://www.avito.ru/{city_part}?q={encoded_query}&s=104"

        if kwargs.get('min_price'):
            url += f"&pmin={int(kwargs['min_price'])}"
        if kwargs.get('max_price'):
            url += f"&pmax={int(kwargs['max_price'])}"
        if page > 1:
            url += f"&p={page}"

        return url

    def _parse_html_advanced(self, html: str, query: str) -> list:
        """–ü–∞—Ä—Å–∏–Ω–≥ HTML (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –¥–ª—è —Ç–µ—Å—Ç–∞)"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            items = []

            # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤
            elements = soup.select('[data-marker="item"]')

            for elem in elements[:20]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                try:
                    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
                    title_elem = elem.select_one('[data-marker="item-title"]')
                    title = title_elem.get_text(strip=True) if title_elem else ""

                    # –¶–µ–Ω–∞
                    price_elem = elem.select_one('[data-marker="item-price"]')
                    price_text = price_elem.get_text(strip=True) if price_elem else ""
                    price = self._parse_price(price_text)

                    # –°—Å—ã–ª–∫–∞
                    link_elem = elem.select_one('a[data-marker="item-title"]')
                    link = link_elem.get('href') if link_elem else ""
                    if link and not link.startswith('http'):
                        link = f"https://www.avito.ru{link}"

                    if title and price > 0:
                        items.append({
                            'name': title[:150],
                            'price': price,
                            'url': link,
                            'query': query,
                            'city': self.city
                        })

                except Exception as e:
                    continue

            return items

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML: {e}")
            return []

    def _parse_price(self, price_text: str) -> int:
        """–ü–∞—Ä—Å–∏—Ç —Ü–µ–Ω—É"""
        try:
            digits = ''.join(filter(str.isdigit, price_text))
            return int(digits) if digits else 0
        except:
            return 0


# üî• –ë–´–°–¢–†–´–ô –¢–ï–°–¢ –ù–û–í–û–ô –°–ò–°–¢–ï–ú–´
if __name__ == "__main__":
    import sys

    # üî• –ó–ê–ì–†–£–ó–ö–ê .env –ü–ï–†–ï–î –¢–ï–°–¢–û–ú
    from dotenv import load_dotenv

    load_dotenv()

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s | %(levelname)-8s | %(name)s | %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler('proxy_rotator.log', encoding='utf-8')
        ]
    )

    print("=" * 70)
    print("üî• –¢–ï–°–¢ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ô –†–û–¢–ê–¶–ò–ò –ü–†–û–ö–°–ò (–¥–∞–Ω–Ω—ã–µ –∏–∑ .env)")
    print("=" * 70)

    # –ü–æ–∫–∞–∂–µ–º –∫–∞–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∑–∏–ª–∏—Å—å
    proxy_data = load_proxy_data_from_env()
    print(f"\nüìã –ó–ê–ì–†–£–ñ–ï–ù–ù–´–ï –ü–†–û–ö–°–ò:")
    for key, value in proxy_data.items():
        if value and 'PASSWORD' not in key and 'PASS' not in key:
            print(f"  ‚úÖ {key}: {value[:20]}...")
        elif value:
            print(f"  ‚úÖ {key}: *** (—Å–∫—Ä—ã—Ç–æ)")

    engine = CloudscraperEngine(city="–ú–æ—Å–∫–≤–∞")

    # –¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
    result = engine.search_items_fast("iPhone", max_pages=1)

    if result and result.get('success'):
        print(f"\nüéâ –£–°–ü–ï–•! –ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {result['total_items']}")
        print(f"üìä –°—Ç—Ä–∞–Ω–∏—Ü –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {result['successful_pages']}/{result['total_pages']}")

        # –ü–æ–∫–∞–∂–µ–º –ø–µ—Ä–≤—ã–π —Ç–æ–≤–∞—Ä
        if result['items']:
            item = result['items'][0]
            print(f"\nüì± –ü—Ä–∏–º–µ—Ä —Ç–æ–≤–∞—Ä–∞:")
            print(f"   –ù–∞–∑–≤–∞–Ω–∏–µ: {item['name'][:80]}...")
            print(f"   –¶–µ–Ω–∞: {item['price']}‚ÇΩ")
            print(f"   –ì–æ—Ä–æ–¥: {item['city']}")
    else:
        print(f"\nüòî –ü–æ–∏—Å–∫ –Ω–µ —É–¥–∞–ª—Å—è")
        print(f"üìã –û—Ç—á–µ—Ç: {engine.proxy_manager.get_status_report()}")

    print("\n" + "=" * 70)