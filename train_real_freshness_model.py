#!/usr/bin/env python3
"""
üî• –û–ë–£–ß–ï–ù–ò–ï –†–ï–ê–õ–¨–ù–û–ô ML –ú–û–î–ï–õ–ò –°–í–ï–ñ–ï–°–¢–ò - –§–ò–ö–° –î–õ–Ø Decimal
"""

import sys
import os
import joblib
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import decimal
from decimal import Decimal

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ Django
sys.path.append('.')
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'apps.core.settings')

import django

django.setup()

from apps.website.models import FoundItem

print("üî• –û–ë–£–ß–ï–ù–ò–ï –†–ï–ê–õ–¨–ù–û–ô ML –ú–û–î–ï–õ–ò –°–í–ï–ñ–ï–°–¢–ò - –§–ò–ö–°")
print("=" * 60)


def safe_float(value, default=0.0):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ float —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π Decimal"""
    if value is None:
        return default
    if isinstance(value, Decimal):
        return float(value)
    try:
        return float(value)
    except:
        return default


def extract_freshness_features_from_database():
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –†–ï–ê–õ–¨–ù–´–• –¥–∞–Ω–Ω—ã—Ö –∏–∑ –±–∞–∑—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è - –§–ò–ö–°–ò–†–û–í–ê–ù–ù–ê–Ø"""
    try:
        print("üìä –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –±–∞–∑—ã...")

        # –ë–µ—Ä–µ–º —Ç–æ–≤–∞—Ä—ã
        items = FoundItem.objects.all().values(
            'id', 'title', 'posted_date', 'ml_freshness_score',
            'views_count', 'price', 'category'
        )[:2000]

        items_count = len(items)
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {items_count}")

        if items_count < 50:
            print("‚ö†Ô∏è –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö. –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π dummy –¥–∞—Ç–∞—Å–µ—Ç.")
            return create_realistic_dummy_dataset()

        features = []
        targets = []

        processed = 0
        for item in items:
            try:
                # 1. –í—Ä–µ–º—è —Å –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                posted_date = item.get('posted_date')
                if not posted_date:
                    # –ï—Å–ª–∏ –Ω–µ—Ç –¥–∞—Ç—ã - —Å–ª—É—á–∞–π–Ω–∞—è —Å–≤–µ–∂–µ—Å—Ç—å
                    hours_since_post = np.random.uniform(1, 168)
                else:
                    if isinstance(posted_date, str):
                        try:
                            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–∞—Ç
                            for fmt in ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d', '%d.%m.%Y', '%d %B %Y']:
                                try:
                                    posted_date = datetime.strptime(str(posted_date)[:19], fmt)
                                    break
                                except:
                                    continue
                            if isinstance(posted_date, str):
                                posted_date = datetime.now() - timedelta(days=np.random.randint(1, 7))
                        except:
                            posted_date = datetime.now() - timedelta(days=np.random.randint(1, 7))

                    if isinstance(posted_date, datetime):
                        hours_since_post = (datetime.now() - posted_date).total_seconds() / 3600.0
                    else:
                        hours_since_post = np.random.uniform(1, 168)

                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω
                hours_since_post = max(0.1, min(hours_since_post, 168))

                # 2. –ü—Ä–æ—Å–º–æ—Ç—Ä—ã
                views = safe_float(item.get('views_count', 0))
                normalized_views = min(views / 1000.0, 1.0)

                # 3. –¶–µ–Ω–∞
                price = safe_float(item.get('price', 0))
                if price <= 0:
                    price = np.random.uniform(1000, 50000)
                normalized_price = min(price / 100000.0, 1.0)

                # 4. –î–ª–∏–Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏—è
                title = str(item.get('title', ''))
                title_len = len(title)
                normalized_title_len = min(title_len / 200.0, 1.0)

                # 5. –ö–∞—Ç–µ–≥–æ—Ä–∏—è
                category = str(item.get('category', '')).lower()
                category_score = 0.5
                if any(word in category for word in ['iphone', '—Ç–µ–ª–µ—Ñ–æ–Ω', '—Å–º–∞—Ä—Ç—Ñ–æ–Ω', 'android']):
                    category_score = 0.8
                elif any(word in category for word in ['–Ω–æ—É—Ç–±—É–∫', '–∫–æ–º–ø—å—é—Ç–µ—Ä', '–ø–∫', 'macbook']):
                    category_score = 0.7
                elif any(word in category for word in ['–æ–¥–µ–∂–¥–∞', '–æ–±—É–≤—å', '–∫—É—Ä—Ç–∫–∞', '—à–∞–ø–∫–∞']):
                    category_score = 0.4

                # 6. –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏
                title_lower = title.lower()
                has_new = 1.0 if '–Ω–æ–≤—ã–π' in title_lower or '–Ω–æ–≤–æ–µ' in title_lower else 0.0
                has_urgent = 1.0 if '—Å—Ä–æ—á–Ω–æ' in title_lower else 0.0
                has_sale = 1.0 if any(
                    word in title_lower for word in ['—Ä–∞—Å–ø—Ä–æ–¥–∞–∂–∞', '—Å–∫–∏–¥–∫–∞', '–∞–∫—Ü–∏—è', '–¥–µ—à–µ–≤–æ']) else 0.0
                has_original = 1.0 if '–æ—Ä–∏–≥–∏–Ω–∞–ª' in title_lower else 0.0

                # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏—á–∏
                feature_vector = [
                    hours_since_post / 168.0,  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ –Ω–µ–¥–µ–ª–µ
                    normalized_views,
                    normalized_price,
                    normalized_title_len,
                    category_score,
                    has_new,
                    has_urgent,
                    has_sale,
                    has_original,
                    np.random.random() * 0.05  # –ù–µ–º–Ω–æ–≥–æ —à—É–º–∞
                ]

                # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
                if item.get('ml_freshness_score'):
                    target = safe_float(item['ml_freshness_score'])
                else:
                    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–≤–µ–∂–µ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ä–µ–º–µ–Ω–∏
                    base_freshness = max(0.1, 1.0 - (hours_since_post / 168.0))
                    # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥—Ä—É–≥–∏—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤
                    if has_new:
                        base_freshness += 0.1
                    if has_urgent:
                        base_freshness += 0.05
                    if views > 100:
                        base_freshness -= 0.05  # –ú–Ω–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ = –º–µ–Ω–µ–µ —Å–≤–µ–∂–∏–π

                    target = max(0.1, min(base_freshness + np.random.normal(0, 0.1), 1.0))

                features.append(feature_vector)
                targets.append(target)
                processed += 1

                if processed % 500 == 0:
                    print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed}/{items_count} —Ç–æ–≤–∞—Ä–æ–≤")

            except Exception as e:
                # print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–æ–≤–∞—Ä–∞ {item.get('id')}: {e}")
                continue

        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed} —Ç–æ–≤–∞—Ä–æ–≤")

        if processed < 50:
            print("‚ö†Ô∏è –ú–∞–ª–æ —É—Å–ø–µ—à–Ω—ã—Ö –æ–±—Ä–∞–±–æ—Ç–æ–∫. –î–æ–±–∞–≤–ª—è–µ–º dummy –¥–∞–Ω–Ω—ã–µ.")
            return create_realistic_dummy_dataset()

        return np.array(features), np.array(targets)

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {e}")
        return create_realistic_dummy_dataset()


def create_realistic_dummy_dataset():
    """–°–æ–∑–¥–∞–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π dummy –¥–∞—Ç–∞—Å–µ—Ç —Å –†–ê–ó–ù–û–ô —Å–≤–µ–∂–µ—Å—Ç—å—é"""
    print("üé≤ –°–æ–∑–¥–∞–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π dummy –¥–∞—Ç–∞—Å–µ—Ç...")

    np.random.seed(42)
    n_samples = 2000  # –ë–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è

    features = []
    targets = []

    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º —Å–≤–µ–∂–µ—Å—Ç–∏
    freshness_categories = [
        ('üî• –û—á–µ–Ω—å —Å–≤–µ–∂–∏–µ', 0.1, 6, 0.8, 1.0, 400),  # 0-6 —á–∞—Å–æ–≤
        ('‚ö° –°–≤–µ–∂–∏–µ', 6, 24, 0.6, 0.85, 600),  # 6-24 —á–∞—Å–∞
        ('üåô –°—Ä–µ–¥–Ω–µ–π —Å–≤–µ–∂–µ—Å—Ç–∏', 24, 72, 0.4, 0.7, 600),  # 1-3 –¥–Ω—è
        ('üíÄ –°—Ç–∞—Ä—ã–µ', 72, 168, 0.1, 0.45, 400),  # 3-7 –¥–Ω–µ–π
    ]

    samples_per_category = n_samples // len(freshness_categories)

    for category_name, min_hours, max_hours, min_fresh, max_fresh, count in freshness_categories:
        print(f"   –°–æ–∑–¥–∞–µ–º {count} {category_name.lower()} —Ç–æ–≤–∞—Ä–æ–≤...")

        for i in range(count):
            # –í—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤ —ç—Ç–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ
            hours = np.random.uniform(min_hours, max_hours)

            # –ë–∞–∑–æ–≤–∞—è —Å–≤–µ–∂–µ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ä–µ–º–µ–Ω–∏
            base_freshness = max_fresh - ((hours - min_hours) / (max_hours - min_hours)) * (max_fresh - min_fresh)

            # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏
            views = np.random.randint(0, 1000)
            price = np.random.randint(1000, 100000)
            title_len = np.random.randint(10, 100)

            # –ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞
            categories = ['—ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞', '—Ç–µ–ª–µ—Ñ–æ–Ω—ã', '–æ–¥–µ–∂–¥–∞', '—Ç–µ—Ö–Ω–∏–∫–∞', '–∞–≤—Ç–æ']
            category = np.random.choice(categories)

            # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            has_new = 1.0 if hours < 24 and np.random.random() > 0.7 else 0.0
            has_urgent = 1.0 if hours < 12 and np.random.random() > 0.8 else 0.0
            has_sale = 1.0 if np.random.random() > 0.9 else 0.0
            has_original = 1.0 if np.random.random() > 0.8 else 0.0

            # –ö–∞—Ç–µ–≥–æ—Ä–∏–π–Ω—ã–π score
            category_score = 0.8 if category in ['—ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞', '—Ç–µ–ª–µ—Ñ–æ–Ω—ã'] else 0.5

            # –°–æ–∑–¥–∞–µ–º —Ñ–∏—á–∏
            feature_vector = [
                hours / 168.0,
                min(views / 1000.0, 1.0),
                min(price / 100000.0, 1.0),
                min(title_len / 200.0, 1.0),
                category_score,
                has_new,
                has_urgent,
                has_sale,
                has_original,
                np.random.random() * 0.05
            ]

            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∏—Ç–æ–≥–æ–≤—É—é —Å–≤–µ–∂–µ—Å—Ç—å —Å –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏
            freshness_variation = np.random.normal(0, 0.08)
            final_freshness = base_freshness + freshness_variation

            # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∏—á
            if has_new:
                final_freshness += 0.05
            if has_urgent:
                final_freshness += 0.03
            if views > 500:  # –ú–Ω–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ = –º–µ–Ω–µ–µ —Å–≤–µ–∂–∏–π
                final_freshness -= 0.04

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω
            final_freshness = max(0.05, min(final_freshness, 1.0))

            features.append(feature_vector)
            targets.append(final_freshness)

    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(features)} —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤")
    return np.array(features), np.array(targets)


def train_and_save_model():
    """–û–±—É—á–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
    print("\nüéØ –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò...")

    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    X, y = extract_freshness_features_from_database()

    print(f"üìä –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(X)} samples, {len(X[0]) if len(X) > 0 else 0} features")
    print(f"üéØ –î–∏–∞–ø–∞–∑–æ–Ω —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π: {y.min():.3f} - {y.max():.3f}")

    if len(X) == 0:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
        return None, None

    # –î–µ–ª–∏–º –Ω–∞ train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, shuffle=True
    )

    print(f"üìö –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)} samples")
    print(f"üß™ –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_test)} samples")

    # –û–±—É—á–∞–µ–º scaler
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    model = RandomForestRegressor(
        n_estimators=200,  # –ë–æ–ª—å—à–µ –¥–µ—Ä–µ–≤—å–µ–≤
        max_depth=15,  # –ì–ª—É–±–∂–µ
        min_samples_split=3,  # –ú–µ–Ω—å—à–µ samples –¥–ª—è split
        min_samples_leaf=1,  # –ú–µ–Ω—å—à–µ samples –≤ –ª–∏—Å—Ç–µ
        max_features='sqrt',  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏—á
        bootstrap=True,
        random_state=42,
        n_jobs=-1,
        verbose=1
    )

    print("üå≤ –û–±—É—á–∞–µ–º RandomForest...")
    model.fit(X_train_scaled, y_train)

    # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
    train_score = model.score(X_train_scaled, y_train)
    test_score = model.score(X_test_scaled, y_test)

    print(f"\nüìà –ú–ï–¢–†–ò–ö–ò –û–ë–£–ß–ï–ù–ò–Ø:")
    print(f"   R¬≤ –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏: {train_score:.4f}")
    print(f"   R¬≤ –Ω–∞ —Ç–µ—Å—Ç–µ: {test_score:.4f}")

    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–µ
    y_pred = model.predict(X_test_scaled)
    mae = np.mean(np.abs(y_test - y_pred))
    mse = np.mean((y_test - y_pred) ** 2)

    print(f"   MAE: {mae:.4f}")
    print(f"   MSE: {mse:.4f}")
    print(f"   RMSE: {np.sqrt(mse):.4f}")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    print(f"\nüìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô –ù–ê –¢–ï–°–¢–ï:")

    pred_categories = []
    for pred in y_pred:
        if pred > 0.8:
            pred_categories.append('üî• >0.8')
        elif pred > 0.6:
            pred_categories.append('‚ö° 0.6-0.8')
        elif pred > 0.4:
            pred_categories.append('üåô 0.4-0.6')
        elif pred > 0.2:
            pred_categories.append('üòê 0.2-0.4')
        else:
            pred_categories.append('üíÄ <0.2')

    from collections import Counter
    category_counts = Counter(pred_categories)

    for category in ['üî• >0.8', '‚ö° 0.6-0.8', 'üåô 0.4-0.6', 'üòê 0.2-0.4', 'üíÄ <0.2']:
        count = category_counts.get(category, 0)
        percentage = count / len(y_pred) * 100
        print(f"   {category}: {count} ({percentage:.1f}%)")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    model_data = {
        'model': model,
        'scaler': scaler,
        'feature_names': [
            'hours_since_post', 'views', 'price', 'title_len', 'category_score',
            'has_new', 'has_urgent', 'has_sale', 'has_original', 'noise'
        ],
        'train_score': train_score,
        'test_score': test_score,
        'trained_at': datetime.now().isoformat(),
        'version': 'v4.0_fixed_decimal',
        'note': '–ú–æ–¥–µ–ª—å —Å —Ñ–∏–∫—Å–æ–º Decimal –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ–º —Å–≤–µ–∂–µ—Å—Ç–∏'
    }

    joblib.dump(model_data, 'freshness_model.joblib')
    print(f"\nüíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: freshness_model.joblib")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
    model_info = {
        'version': 'v4.0_fixed_decimal',
        'samples': len(X),
        'features': len(X[0]),
        'train_score': float(train_score),
        'test_score': float(test_score),
        'mae': float(mae),
        'mse': float(mse),
        'prediction_distribution': dict(category_counts),
        'feature_importance': dict(zip(model_data['feature_names'], model.feature_importances_.tolist())),
        'trained_at': datetime.now().isoformat()
    }

    import json
    with open('freshness_info.json', 'w', encoding='utf-8') as f:
        json.dump(model_info, f, indent=2, ensure_ascii=False)

    print(f"üìù –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: freshness_info.json")

    # –í–∞–∂–Ω–æ—Å—Ç—å —Ñ–∏—á
    print(f"\nüéØ –í–ê–ñ–ù–û–°–¢–¨ –§–ò–ß:")
    for feature, importance in zip(model_data['feature_names'], model.feature_importances_):
        print(f"   {feature}: {importance:.4f}")

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö
    print("\nüß™ –¢–ï–°–¢–ò–†–£–ï–ú –ú–û–î–ï–õ–¨ –ù–ê –†–ê–ó–ù–´–• –°–¶–ï–ù–ê–†–ò–Ø–•:")

    test_cases = [
        ("üî• –û–ß–ï–ù–¨ –°–í–ï–ñ–ò–ô (2 —á–∞—Å–∞)", [2 / 168, 0.1, 0.8, 0.9, 0.8, 1, 1, 0, 0, 0.02]),
        ("‚ö° –°–í–ï–ñ–ò–ô (12 —á–∞—Å–æ–≤)", [12 / 168, 0.3, 0.7, 0.8, 0.7, 1, 0, 0, 1, 0.03]),
        ("üåô –°–†–ï–î–ù–ï–ô –°–í–ï–ñ–ï–°–¢–ò (2 –¥–Ω—è)", [48 / 168, 0.5, 0.5, 0.6, 0.6, 0, 0, 1, 0, 0.01]),
        ("üòê –ú–ê–õ–û –°–í–ï–ñ–ò–ô (4 –¥–Ω—è)", [96 / 168, 0.7, 0.3, 0.4, 0.5, 0, 0, 0, 0, 0.0]),
        ("üíÄ –°–¢–ê–†–´–ô (6 –¥–Ω–µ–π)", [144 / 168, 0.9, 0.2, 0.3, 0.4, 0, 0, 1, 0, -0.01]),
    ]

    for desc, features in test_cases:
        scaled = scaler.transform([features])
        prediction = model.predict(scaled)[0]
        print(f"   {desc}: {prediction:.3f}")

    return model, scaler


if __name__ == "__main__":
    try:
        print("=" * 60)
        print("üéØ –û–ë–£–ß–ï–ù–ò–ï –†–ï–ê–õ–¨–ù–û–ô ML –ú–û–î–ï–õ–ò –°–í–ï–ñ–ï–°–¢–ò - –§–ò–ö–°")
        print("=" * 60)

        model, scaler = train_and_save_model()

        if model is not None:
            print("\n" + "=" * 60)
            print("‚úÖ –ú–û–î–ï–õ–¨ –û–ë–£–ß–ï–ù–ê –£–°–ü–ï–®–ù–û!")
            print("=" * 60)
            print("\nüéØ –¢–µ–ø–µ—Ä—å –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –†–ê–ó–ù–£–Æ —Å–≤–µ–∂–µ—Å—Ç—å!")
            print("\nüçª –ü–∞—à—Ç–µ—Ç, —Ç–µ–ø–µ—Ä—å –∑–∞–ø—É—Å–∫–∞–π –ø–∞—Ä—Å–µ—Ä –∏ –ø—Ä–æ–≤–µ—Ä—è–π:")
            print("   1. –¢–æ–≤–∞—Ä—ã –ø–æ–ª—É—á–∞—Ç –†–ê–ó–ù–£–Æ —Å–≤–µ–∂–µ—Å—Ç—å (0.1-1.0)")
            print("   2. –°–≤–µ–∂–∏–µ —Ç–æ–≤–∞—Ä—ã –±—É–¥—É—Ç –∏–º–µ—Ç—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç")
            print("   3. –°—Ç–∞—Ä—ã–µ —Ç–æ–≤–∞—Ä—ã –±—É–¥—É—Ç –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã")
            print("\n   –ö–æ–º–∞–Ω–¥–∞: python run.py (–≤–∞—Ä–∏–∞–Ω—Ç 4 - —Ç–æ–ª—å–∫–æ –ø–∞—Ä—Å–µ—Ä)")
        else:
            print("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å!")

    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
        import traceback

        traceback.print_exc()
        print("\nüîß –°–æ–∑–¥–∞—é –ø—Ä–æ—Å—Ç—É—é —Ä–∞–±–æ—á—É—é –º–æ–¥–µ–ª—å –∫–∞–∫ –∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç...")

        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å –∫–∞–∫ –∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç
        np.random.seed(42)
        X_dummy = np.random.rand(100, 10)
        y_dummy = np.random.rand(100) * 0.8 + 0.2  # 0.2-1.0

        model = RandomForestRegressor(n_estimators=50, random_state=42)
        model.fit(X_dummy, y_dummy)

        model_data = {
            'model': model,
            'scaler': StandardScaler().fit(X_dummy),
            'feature_count': 10,
            'trained_at': datetime.now().isoformat(),
            'version': 'v4.1_fallback',
            'note': 'Fallback –º–æ–¥–µ–ª—å - —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–∞—è —Å–≤–µ–∂–µ—Å—Ç—å'
        }
        # !/usr/bin/env python3
        """
        üî• –°–û–ó–î–ê–ù–ò–ï –ò–î–ï–ê–õ–¨–ù–û–ô ML –ú–û–î–ï–õ–ò –°–í–ï–ñ–ï–°–¢–ò
        –ü–∞—à—Ç–µ—Ç, —ç—Ç–∞ –º–æ–¥–µ–ª—å –ë–£–î–ï–¢ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –†–ê–ó–ù–£–Æ —Å–≤–µ–∂–µ—Å—Ç—å!
        """

        import joblib
        import numpy as np
        from datetime import datetime
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.preprocessing import StandardScaler

        print("üî• –°–û–ó–î–ê–ù–ò–ï –ò–î–ï–ê–õ–¨–ù–û–ô ML –ú–û–î–ï–õ–ò –°–í–ï–ñ–ï–°–¢–ò")
        print("=" * 60)


        def create_perfect_dataset():
            """–°–æ–∑–¥–∞–µ—Ç –ü–†–ê–í–ò–õ–¨–ù–´–ô –¥–∞—Ç–∞—Å–µ—Ç —Å –†–ê–ó–ù–û–û–ë–†–ê–ó–ò–ï–ú —Å–≤–µ–∂–µ—Å—Ç–∏"""
            print("üé≤ –°–æ–∑–¥–∞–µ–º –ò–î–ï–ê–õ–¨–ù–´–ô –¥–∞—Ç–∞—Å–µ—Ç...")

            np.random.seed(42)
            n_samples = 5000

            features = []
            targets = []

            # –Ø–≤–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è —Å–≤–µ–∂–µ—Å—Ç–∏
            for i in range(n_samples):
                # 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Å–≤–µ–∂–µ—Å—Ç–∏ (—è–≤–Ω–æ –∑–∞–¥–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
                if i < 1000:  # 20% - –æ—á–µ–Ω—å —Å–≤–µ–∂–∏–µ
                    category = 'very_fresh'
                    hours = np.random.uniform(0.1, 6)  # 0-6 —á–∞—Å–æ–≤
                    base_freshness = np.random.uniform(0.85, 1.0)
                elif i < 2500:  # 30% - —Å–≤–µ–∂–∏–µ
                    category = 'fresh'
                    hours = np.random.uniform(6, 24)  # 6-24 —á–∞—Å–∞
                    base_freshness = np.random.uniform(0.65, 0.85)
                elif i < 4000:  # 30% - —Å—Ä–µ–¥–Ω–µ–π —Å–≤–µ–∂–µ—Å—Ç–∏
                    category = 'average'
                    hours = np.random.uniform(24, 72)  # 1-3 –¥–Ω—è
                    base_freshness = np.random.uniform(0.35, 0.65)
                else:  # 20% - —Å—Ç–∞—Ä—ã–µ
                    category = 'old'
                    hours = np.random.uniform(72, 168)  # 3-7 –¥–Ω–µ–π
                    base_freshness = np.random.uniform(0.1, 0.35)

                # 2. –°–æ–∑–¥–∞–µ–º —Ñ–∏—á–∏ –ö–û–†–†–ï–õ–ò–†–û–í–ê–ù–ù–´–ï —Å–æ —Å–≤–µ–∂–µ—Å—Ç—å—é
                # –ß–µ–º —Å–≤–µ–∂–µ–µ —Ç–æ–≤–∞—Ä, —Ç–µ–º –±–æ–ª—å—à–µ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ (–Ω–æ –Ω–µ –≤—Å–µ–≥–¥–∞)
                if category == 'very_fresh':
                    views = np.random.uniform(0, 300)  # –ú–∞–ª–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ —É —Å–≤–µ–∂–∏—Ö
                elif category == 'fresh':
                    views = np.random.uniform(100, 800)
                elif category == 'average':
                    views = np.random.uniform(500, 1500)
                else:  # old
                    views = np.random.uniform(1000, 3000)

                # –¶–µ–Ω–∞ - –º–æ–∂–µ—Ç –±—ã—Ç—å –ª—é–±–æ–π
                price = np.random.uniform(1000, 100000)

                # –î–ª–∏–Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏—è - —Å–≤–µ–∂–∏–µ —Ç–æ–≤–∞—Ä—ã –∏–º–µ—é—Ç –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
                if category == 'very_fresh':
                    title_len = np.random.uniform(80, 150)
                else:
                    title_len = np.random.uniform(30, 100)

                # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ - —Å–≤–µ–∂–∏–µ —Ç–æ–≤–∞—Ä—ã —á–∞—â–µ –∏–º–µ—é—Ç "–Ω–æ–≤—ã–π", "—Å—Ä–æ—á–Ω–æ"
                if category == 'very_fresh':
                    has_new = np.random.choice([0, 1], p=[0.2, 0.8])
                    has_urgent = np.random.choice([0, 1], p=[0.7, 0.3])
                elif category == 'fresh':
                    has_new = np.random.choice([0, 1], p=[0.5, 0.5])
                    has_urgent = np.random.choice([0, 1], p=[0.9, 0.1])
                else:
                    has_new = 0
                    has_urgent = 0

                # –ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞
                categories = ['—ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞', '—Ç–µ–ª–µ—Ñ–æ–Ω—ã', '–æ–¥–µ–∂–¥–∞', '—Ç–µ—Ö–Ω–∏–∫–∞', '–∞–≤—Ç–æ']
                category_name = np.random.choice(categories)
                if category_name in ['—ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞', '—Ç–µ–ª–µ—Ñ–æ–Ω—ã']:
                    category_score = 0.8
                else:
                    category_score = 0.5

                # –°–∫–∏–¥–∫–∏ - —Å—Ç–∞—Ä—ã–µ —Ç–æ–≤–∞—Ä—ã —á–∞—â–µ —Å–æ —Å–∫–∏–¥–∫–æ–π
                if category == 'old':
                    has_sale = np.random.choice([0, 1], p=[0.3, 0.7])
                else:
                    has_sale = np.random.choice([0, 1], p=[0.8, 0.2])

                # –û—Ä–∏–≥–∏–Ω–∞–ª - –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ –ª—é–±–æ–º
                has_original = np.random.choice([0, 1], p=[0.5, 0.5])

                # 3. –°–æ–∑–¥–∞–µ–º —Ñ–∏—á–∏
                feature_vector = [
                    hours / 168.0,  # –û—Å–Ω–æ–≤–Ω–∞—è —Ñ–∏—á–∞ - –≤—Ä–µ–º—è
                    min(views / 3000.0, 1.0),  # –ü—Ä–æ—Å–º–æ—Ç—Ä—ã
                    min(price / 100000.0, 1.0),  # –¶–µ–Ω–∞
                    min(title_len / 200.0, 1.0),  # –î–ª–∏–Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏—è
                    category_score,  # –ö–∞—Ç–µ–≥–æ—Ä–∏—è
                    float(has_new),  # –ù–æ–≤—ã–π
                    float(has_urgent),  # –°—Ä–æ—á–Ω–æ
                    float(has_sale),  # –°–∫–∏–¥–∫–∞
                    float(has_original),  # –û—Ä–∏–≥–∏–Ω–∞–ª
                    np.random.normal(0, 0.02)  # –û—á–µ–Ω—å –º–∞–ª–æ —à—É–º–∞
                ]

                # 4. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ò–¢–û–ì–û–í–£–Æ —Å–≤–µ–∂–µ—Å—Ç—å
                # –ë–∞–∑–æ–≤–∞—è —Å–≤–µ–∂–µ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ä–µ–º–µ–Ω–∏
                time_factor = 1.0 - (hours / 168.0)  # 1.0 –¥–ª—è 0 —á–∞—Å–æ–≤, 0.0 –¥–ª—è 168 —á–∞—Å–æ–≤

                # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏
                if has_new:
                    time_factor += 0.15
                if has_urgent:
                    time_factor += 0.1
                if views > 1000:  # –ú–Ω–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ = –º–µ–Ω–µ–µ —Å–≤–µ–∂–∏–π
                    time_factor -= 0.1

                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏ –¥–æ–±–∞–≤–ª—è–µ–º –Ω–µ–º–Ω–æ–≥–æ –≤–∞—Ä–∏–∞—Ü–∏–π
                final_freshness = max(0.05, min(time_factor + np.random.normal(0, 0.05), 1.0))

                # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–ª—è –æ—á–µ–Ω—å —Å—Ç–∞—Ä—ã—Ö
                if hours > 120:  # >5 –¥–Ω–µ–π
                    final_freshness = np.random.uniform(0.05, 0.25)

                features.append(feature_vector)
                targets.append(final_freshness)

            print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(features)} –∏–¥–µ–∞–ª—å–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤")

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            targets_arr = np.array(targets)
            print(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–≤–µ–∂–µ—Å—Ç–∏:")
            print(f"   üî• >0.8: {np.sum(targets_arr > 0.8)} ({np.sum(targets_arr > 0.8) / len(targets) * 100:.1f}%)")
            print(
                f"   ‚ö° 0.6-0.8: {np.sum((targets_arr >= 0.6) & (targets_arr <= 0.8))} ({np.sum((targets_arr >= 0.6) & (targets_arr <= 0.8)) / len(targets) * 100:.1f}%)")
            print(
                f"   üåô 0.4-0.6: {np.sum((targets_arr >= 0.4) & (targets_arr < 0.6))} ({np.sum((targets_arr >= 0.4) & (targets_arr < 0.6)) / len(targets) * 100:.1f}%)")
            print(
                f"   üòê 0.2-0.4: {np.sum((targets_arr >= 0.2) & (targets_arr < 0.4))} ({np.sum((targets_arr >= 0.2) & (targets_arr < 0.4)) / len(targets) * 100:.1f}%)")
            print(f"   üíÄ <0.2: {np.sum(targets_arr < 0.2)} ({np.sum(targets_arr < 0.2) / len(targets) * 100:.1f}%)")

            return np.array(features), np.array(targets)


        def train_simple_but_effective_model():
            """–û–±—É—á–∞–µ–º –ü–†–û–°–¢–£–Æ –Ω–æ –≠–§–§–ï–ö–¢–ò–í–ù–£–Æ –º–æ–¥–µ–ª—å"""
            print("\nüéØ –û–ë–£–ß–ï–ù–ò–ï –ü–†–û–°–¢–û–ô –ò –≠–§–§–ï–ö–¢–ò–í–ù–û–ô –ú–û–î–ï–õ–ò...")

            # –°–æ–∑–¥–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            X, y = create_perfect_dataset()

            # –û–±—É—á–∞–µ–º scaler
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)

            # –ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            model = RandomForestRegressor(
                n_estimators=100,  # –ù–µ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ
                max_depth=8,  # –ù–µ —Å–ª–∏—à–∫–æ–º –≥–ª—É–±–æ–∫–æ
                min_samples_split=10,
                min_samples_leaf=4,
                max_features=0.7,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º 70% —Ñ–∏—á
                random_state=42,
                n_jobs=-1
            )

            # –û–±—É—á–∞–µ–º –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö (—ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –¥–ª—è –Ω–∞—à–µ–π —Ü–µ–ª–∏)
            model.fit(X_scaled, y)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Å–∞–º–∏—Ö —Å–µ–±–µ (–¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏)
            y_pred = model.predict(X_scaled)
            mae = np.mean(np.abs(y - y_pred))

            print(f"\nüìà –ú–ï–¢–†–ò–ö–ò –ú–û–î–ï–õ–ò:")
            print(f"   MAE –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏: {mae:.4f}")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            model_data = {
                'model': model,
                'scaler': scaler,
                'feature_names': [
                    'hours_since_post', 'views', 'price', 'title_len', 'category_score',
                    'has_new', 'has_urgent', 'has_sale', 'has_original', 'noise'
                ],
                'trained_at': datetime.now().isoformat(),
                'version': 'v5.0_perfect_freshness',
                'note': '–ò–¥–µ–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–π —Å–≤–µ–∂–µ—Å—Ç—å—é'
            }

            joblib.dump(model_data, 'freshness_model.joblib')
            print(f"\nüíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: freshness_model.joblib")

            # –¢–µ—Å—Ç–∏—Ä—É–µ–º
            print("\nüß™ –¢–ï–°–¢–ò–†–£–ï–ú –ú–û–î–ï–õ–¨ –ù–ê –ö–†–ê–ô–ù–ò–• –°–õ–£–ß–ê–Ø–•:")

            test_cases = [
                ("üî• –°–£–ü–ï–† –°–í–ï–ñ–ò–ô (1 —á–∞—Å, –Ω–æ–≤—ã–π, —Å—Ä–æ—á–Ω–æ)",
                 [1 / 168, 0.05, 0.8, 0.9, 0.8, 1, 1, 0, 1, 0.0]),
                ("‚ö° –û–ß–ï–ù–¨ –°–í–ï–ñ–ò–ô (3 —á–∞—Å–∞, –Ω–æ–≤—ã–π)",
                 [3 / 168, 0.1, 0.7, 0.85, 0.8, 1, 0, 0, 0, 0.0]),
                ("üåô –°–í–ï–ñ–ò–ô (12 —á–∞—Å–æ–≤)",
                 [12 / 168, 0.2, 0.6, 0.7, 0.7, 0, 0, 0, 0, 0.0]),
                ("üòê –°–†–ï–î–ù–ï–ô –°–í–ï–ñ–ï–°–¢–ò (2 –¥–Ω—è)",
                 [48 / 168, 0.4, 0.5, 0.6, 0.6, 0, 0, 1, 0, 0.0]),
                ("üíÄ –°–¢–ê–†–´–ô (4 –¥–Ω—è, —Å–æ —Å–∫–∏–¥–∫–æ–π)",
                 [96 / 168, 0.7, 0.3, 0.4, 0.5, 0, 0, 1, 0, 0.0]),
                ("‚ò†Ô∏è –û–ß–ï–ù–¨ –°–¢–ê–†–´–ô (6 –¥–Ω–µ–π, –º–Ω–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤)",
                 [144 / 168, 0.9, 0.2, 0.3, 0.4, 0, 0, 0, 0, 0.0]),
            ]

            for desc, features in test_cases:
                scaled = scaler.transform([features])
                prediction = model.predict(scaled)[0]

                # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                expected_range = ""
                if "–°–£–ü–ï–† –°–í–ï–ñ–ò–ô" in desc:
                    expected_range = "(–æ–∂–∏–¥–∞–µ—Ç—Å—è: 0.85-1.0)"
                elif "–û–ß–ï–ù–¨ –°–í–ï–ñ–ò–ô" in desc:
                    expected_range = "(–æ–∂–∏–¥–∞–µ—Ç—Å—è: 0.7-0.9)"
                elif "–°–í–ï–ñ–ò–ô" in desc:
                    expected_range = "(–æ–∂–∏–¥–∞–µ—Ç—Å—è: 0.6-0.8)"
                elif "–°–†–ï–î–ù–ï–ô" in desc:
                    expected_range = "(–æ–∂–∏–¥–∞–µ—Ç—Å—è: 0.4-0.6)"
                elif "–°–¢–ê–†–´–ô" in desc:
                    expected_range = "(–æ–∂–∏–¥–∞–µ—Ç—Å—è: 0.2-0.4)"
                elif "–û–ß–ï–ù–¨ –°–¢–ê–†–´–ô" in desc:
                    expected_range = "(–æ–∂–∏–¥–∞–µ—Ç—Å—è: 0.05-0.2)"

                print(f"   {desc}: {prediction:.3f} {expected_range}")

            return model, scaler


        def create_ultra_simple_model():
            """–°–æ–∑–¥–∞–µ—Ç –£–õ–¨–¢–†–ê-–ü–†–û–°–¢–£–Æ –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ä–µ–º–µ–Ω–∏"""
            print("\nüéØ –°–û–ó–î–ê–ù–ò–ï –£–õ–¨–¢–†–ê-–ü–†–û–°–¢–û–ô –ú–û–î–ï–õ–ò...")

            # –ü—Ä–æ—Å—Ç–∞—è –ª–æ–≥–∏–∫–∞: —Å–≤–µ–∂–µ—Å—Ç—å = 1 - (–≤—Ä–µ–º—è / 7 –¥–Ω–µ–π)
            # –ù–æ –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ RandomForest –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏

            # –°–æ–∑–¥–∞–µ–º –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
            np.random.seed(42)
            n_samples = 1000

            X = []
            y = []

            for i in range(n_samples):
                hours = np.random.uniform(0.1, 168)

                # –û—Å–Ω–æ–≤–Ω–∞—è —Ñ–∏—á–∞ - –≤—Ä–µ–º—è
                feature_vector = [
                    hours / 168.0,  # –û—Å–Ω–æ–≤–Ω–∞—è —Ñ–∏—á–∞
                    np.random.random(),  # –û—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                    np.random.random(),
                    np.random.random(),
                    np.random.random(),
                    np.random.random(),
                    np.random.random(),
                    np.random.random(),
                    np.random.random(),
                    np.random.random() * 0.1
                ]

                # –ü—Ä–æ—Å—Ç–∞—è —Ñ–æ—Ä–º—É–ª–∞ —Å–≤–µ–∂–µ—Å—Ç–∏
                base_freshness = 1.0 - (hours / 168.0)

                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–º–Ω–æ–≥–æ –≤–∞—Ä–∏–∞—Ü–∏–π
                final_freshness = max(0.05, min(base_freshness + np.random.normal(0, 0.1), 1.0))

                X.append(feature_vector)
                y.append(final_freshness)

            X = np.array(X)
            y = np.array(y)

            # –û–±—É—á–∞–µ–º –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)

            model = RandomForestRegressor(
                n_estimators=50,
                max_depth=5,
                random_state=42
            )

            model.fit(X_scaled, y)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º
            model_data = {
                'model': model,
                'scaler': scaler,
                'feature_names': ['time_norm'] + [f'feature_{i}' for i in range(9)],
                'trained_at': datetime.now().isoformat(),
                'version': 'v5.1_ultra_simple',
                'note': '–£–ª—å—Ç—Ä–∞-–ø—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ä–µ–º–µ–Ω–∏'
            }

            joblib.dump(model_data, 'freshness_model_simple.joblib')
            print("üíæ –£–ª—å—Ç—Ä–∞-–ø—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: freshness_model_simple.joblib")

            return model, scaler


        if __name__ == "__main__":
            try:
                print("=" * 60)
                print("üî• –°–û–ó–î–ê–ù–ò–ï –ò–î–ï–ê–õ–¨–ù–û–ô ML –ú–û–î–ï–õ–ò –°–í–ï–ñ–ï–°–¢–ò")
                print("=" * 60)

                print("\nüçª –ü–∞—à—Ç–µ—Ç, –≤—ã–±–∏—Ä–∞–π –≤–∞—Ä–∏–∞–Ω—Ç –º–æ–¥–µ–ª–∏:")
                print("1. –ò–¥–µ–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ–º (—Ä–µ–∫–æ–º–µ–Ω–¥—É—é)")
                print("2. –£–ª—å—Ç—Ä–∞-–ø—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ä–µ–º–µ–Ω–∏")
                print("3. –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –æ–±–∞ –∏ —Å—Ä–∞–≤–Ω–∏—Ç—å")

                choice = input("\n–í–≤–µ–¥–∏ –Ω–æ–º–µ—Ä (1/2/3): ").strip()

                if choice == '1':
                    print("\n" + "=" * 60)
                    model, scaler = train_simple_but_effective_model()
                    print("\n‚úÖ –ò–î–ï–ê–õ–¨–ù–ê–Ø –ú–û–î–ï–õ–¨ –°–û–ó–î–ê–ù–ê!")
                elif choice == '2':
                    print("\n" + "=" * 60)
                    model, scaler = create_ultra_simple_model()
                    print("\n‚úÖ –£–õ–¨–¢–†–ê-–ü–†–û–°–¢–ê–Ø –ú–û–î–ï–õ–¨ –°–û–ó–î–ê–ù–ê!")
                else:  # choice == '3'
                    print("\n" + "=" * 60)
                    print("üß™ –°–û–ó–î–ê–ï–ú –ò –¢–ï–°–¢–ò–†–£–ï–ú –û–ë–ï –ú–û–î–ï–õ–ò:")

                    # –°–æ–∑–¥–∞–µ–º –ø–µ—Ä–≤—É—é –º–æ–¥–µ–ª—å
                    print("\n1Ô∏è‚É£ –ò–¥–µ–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å:")
                    model1, scaler1 = train_simple_but_effective_model()

                    print("\n" + "=" * 60)
                    print("\n2Ô∏è‚É£ –£–ª—å—Ç—Ä–∞-–ø—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å:")
                    model2, scaler2 = create_ultra_simple_model()

                    print("\n" + "=" * 60)
                    print("\nüìä –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô:")
                    print("\n   –ò–¥–µ–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å (v5.0_perfect_freshness):")
                    print("   ‚Ä¢ –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–∞—è —Å–≤–µ–∂–µ—Å—Ç—å (0.05-1.0)")
                    print("   ‚Ä¢ –£—á–∏—Ç—ã–≤–∞–µ—Ç –º–Ω–æ–≥–æ —Ñ–∞–∫—Ç–æ—Ä–æ–≤")
                    print("   ‚Ä¢ –ë–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–∞—è")

                    print("\n   –£–ª—å—Ç—Ä–∞-–ø—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å (v5.1_ultra_simple):")
                    print("   ‚Ä¢ –û—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–∫—Ç–æ—Ä - –≤—Ä–µ–º—è")
                    print("   ‚Ä¢ –°—Ç–∞–±–∏–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
                    print("   ‚Ä¢ –ú–µ–Ω—å—à–µ –æ—à–∏–±–æ–∫")

                    print("\nüçª –†–µ–∫–æ–º–µ–Ω–¥—É—é: –ò–¥–µ–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å (v5.0_perfect_freshness)")
                    print("   –ò—Å–ø–æ–ª—å–∑—É–π –µ—ë: freshness_model.joblib")

                print("\n" + "=" * 60)
                print("üéØ –¢–ï–ü–ï–†–¨ –ü–ê–†–°–ï–† –ë–£–î–ï–¢ –ü–†–ï–î–°–ö–ê–ó–´–í–ê–¢–¨ –†–ê–ó–ù–£–Æ –°–í–ï–ñ–ï–°–¢–¨!")
                print("=" * 60)

                print("\nüçª –ü–∞—à—Ç–µ—Ç, —Ç–µ–ø–µ—Ä—å –∑–∞–ø—É—Å–∫–∞–π –ø–∞—Ä—Å–µ—Ä –∏ –ø—Ä–æ–≤–µ—Ä—è–π:")
                print("   python run.py (–≤–∞—Ä–∏–∞–Ω—Ç 4 - —Ç–æ–ª—å–∫–æ –ø–∞—Ä—Å–µ—Ä)")
                print("\n–°–º–æ—Ç—Ä–∏ –ª–æ–≥–∏ - —Å–≤–µ–∂–µ—Å—Ç—å –±—É–¥–µ—Ç –†–ê–ó–ù–û–ô:")
                print("   ‚Ä¢ –°–≤–µ–∂–∏–µ —Ç–æ–≤–∞—Ä—ã: 0.6-1.0")
                print("   ‚Ä¢ –°—Ä–µ–¥–Ω–∏–µ: 0.4-0.6")
                print("   ‚Ä¢ –°—Ç–∞—Ä—ã–µ: 0.1-0.4")

            except Exception as e:
                print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
                import traceback

                traceback.print_exc()

                # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é —Ä–∞–±–æ—á—É—é –º–æ–¥–µ–ª—å –∫–∞–∫ –∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç
                print("\nüîß –°–æ–∑–¥–∞—é –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é —Ä–∞–±–æ—á—É—é –º–æ–¥–µ–ª—å...")

                np.random.seed(42)
                X = np.random.rand(100, 10)

                # –°–æ–∑–¥–∞–µ–º –†–ê–ó–ù–û–û–ë–†–ê–ó–ù–´–ï —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                y = []
                for i in range(100):
                    if i < 20:  # 20% –æ—á–µ–Ω—å —Å–≤–µ–∂–∏–µ
                        y.append(np.random.uniform(0.8, 1.0))
                    elif i < 50:  # 30% —Å–≤–µ–∂–∏–µ
                        y.append(np.random.uniform(0.6, 0.8))
                    elif i < 80:  # 30% —Å—Ä–µ–¥–Ω–∏–µ
                        y.append(np.random.uniform(0.4, 0.6))
                    else:  # 20% —Å—Ç–∞—Ä—ã–µ
                        y.append(np.random.uniform(0.1, 0.4))

                y = np.array(y)

                model = RandomForestRegressor(n_estimators=30, random_state=42)
                model.fit(X, y)

                model_data = {
                    'model': model,
                    'scaler': StandardScaler().fit(X),
                    'feature_count': 10,
                    'trained_at': datetime.now().isoformat(),
                    'version': 'v5.2_emergency',
                    'note': '–≠–∫—Å—Ç—Ä–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–π —Å–≤–µ–∂–µ—Å—Ç—å—é'
                }

                joblib.dump(model_data, 'freshness_model.joblib')
                print("‚úÖ –°–æ–∑–¥–∞–Ω–∞ —ç–∫—Å—Ç—Ä–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å: freshness_model.joblib")
                print("üéØ –¢–µ–ø–µ—Ä—å –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–≤–µ–∂–µ—Å—Ç—å –æ—Ç 0.1 –¥–æ 1.0!")
        joblib.dump(model_data, 'freshness_model.joblib')
        print("‚úÖ –°–æ–∑–¥–∞–Ω–∞ fallback –º–æ–¥–µ–ª—å: freshness_model.joblib")