import logging
import numpy as np
import pandas as pd
import asyncio
import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from collections import defaultdict, deque
import sqlite3
import hashlib
from dataclasses import dataclass, asdict
import joblib

logger = logging.getLogger('parser.ai')


@dataclass
class LearningEpisode:
    timestamp: str
    prediction_type: str
    features: Dict[str, Any]
    prediction: float
    actual_result: Optional[float]
    error: Optional[float]
    confidence: float
    model_version: str
    context: Dict[str, Any]


@dataclass
class ModelPerformance:
    version: str
    training_samples: int
    metrics: Dict[str, float]
    created_at: str
    last_used: str


class LearningSystem:
    def __init__(self, db_path="vision_knowledge.db"):
        self.db_path = db_path
        self.learning_episodes = deque(maxlen=5000)  # üß† –•—Ä–∞–Ω–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5000 —ç–ø–∏–∑–æ–¥–æ–≤
        self.model_versions = {}
        self.performance_metrics = defaultdict(list)
        self.feature_importance = {}
        self.adaptation_rules = {}

        # üéØ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
        self.learning_config = {
            'retrain_interval': 100,  # –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 100 –Ω–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
            'min_training_samples': 50,
            'validation_split': 0.2,
            'performance_threshold': 0.7,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è –º–æ–¥–µ–ª–∏
            'adaptive_learning_rate': 0.1,
            'feature_analysis_interval': 50
        }

        # üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã
        self.system_stats = {
            'total_episodes': 0,
            'successful_predictions': 0,
            'failed_predictions': 0,
            'avg_error': 0.0,
            'learning_progress': 0.0,
            'last_retraining': None,
            'active_models': 0
        }

        # üî• –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
        self._initialize_models()

    def _initialize_models(self):
        """üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞—á–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –æ–±—É—á–µ–Ω–∏—è"""
        try:
            # –ú–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ü–µ–Ω
            self.model_versions['price_predictor_v1'] = ModelPerformance(
                version="price_predictor_v1",
                training_samples=0,
                metrics={'mae': 0.0, 'mse': 0.0, 'r2': 0.0},
                created_at=datetime.now().isoformat(),
                last_used=datetime.now().isoformat()
            )

            # –ú–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–¥–µ–ª–æ–∫
            self.model_versions['quality_assessor_v1'] = ModelPerformance(
                version="quality_assessor_v1",
                training_samples=0,
                metrics={'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0},
                created_at=datetime.now().isoformat(),
                last_used=datetime.now().isoformat()
            )

            # –ú–æ–¥–µ–ª—å –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
            self.model_versions['query_optimizer_v1'] = ModelPerformance(
                version="query_optimizer_v1",
                training_samples=0,
                metrics={'success_rate': 0.0, 'improvement': 0.0},
                created_at=datetime.now().isoformat(),
                last_used=datetime.now().isoformat()
            )

            self.system_stats['active_models'] = len(self.model_versions)
            logger.info("üß† –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –º–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω–∏—è")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π: {e}")

    async def collect_feedback(self, prediction: float, actual_result: Optional[float],
                               features: Dict[str, Any], prediction_type: str = "price",
                               confidence: float = 0.5, context: Dict[str, Any] = None):
        """üì• –°–±–æ—Ä –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π"""
        try:
            # –†–∞—Å—á–µ—Ç –æ—à–∏–±–∫–∏ –µ—Å–ª–∏ –µ—Å—Ç—å —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            error = None
            if actual_result is not None:
                error = abs(prediction - actual_result)

                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
                if error < (prediction * 0.1):  # –û—à–∏–±–∫–∞ –º–µ–Ω–µ–µ 10%
                    self.system_stats['successful_predictions'] += 1
                else:
                    self.system_stats['failed_predictions'] += 1

            # –°–æ–∑–¥–∞–Ω–∏–µ —ç–ø–∏–∑–æ–¥–∞ –æ–±—É—á–µ–Ω–∏—è
            episode = LearningEpisode(
                timestamp=datetime.now().isoformat(),
                prediction_type=prediction_type,
                features=features,
                prediction=prediction,
                actual_result=actual_result,
                error=error,
                confidence=confidence,
                model_version=f"{prediction_type}_v1",
                context=context or {}
            )

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–ø–∏–∑–æ–¥–∞
            self.learning_episodes.append(episode)
            self.system_stats['total_episodes'] += 1

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–π –æ—à–∏–±–∫–∏
            if error is not None:
                errors = [e.error for e in self.learning_episodes if e.error is not None]
                if errors:
                    self.system_stats['avg_error'] = np.mean(errors)

            # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            if self.system_stats['total_episodes'] % self.learning_config['feature_analysis_interval'] == 0:
                await self._analyze_feature_importance()

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
            if (self.system_stats['total_episodes'] % self.learning_config['retrain_interval'] == 0 and
                    self.system_stats['total_episodes'] >= self.learning_config['min_training_samples']):
                await self.retrain_models_advanced()

            # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø—Ä–∞–≤–∏–ª –Ω–∞ –ª–µ—Ç—É
            await self._adaptive_rule_optimization(episode)

            logger.debug(f"üì• –°–æ–±—Ä–∞–Ω —ç–ø–∏–∑–æ–¥ –æ–±—É—á–µ–Ω–∏—è: {prediction_type} (–æ—à–∏–±–∫–∞: {error})")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏: {e}")

    async def retrain_models_advanced(self):
        """üîÑ –ü–ï–†–ï–î–û–í–û–ï –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–ï –º–æ–¥–µ–ª–µ–π —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ —Ç–µ—Ö–Ω–∏–∫–∞–º–∏"""
        try:
            logger.info("üîÑ –ó–∞–ø—É—Å–∫ –ø–µ—Ä–µ–¥–æ–≤–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π...")

            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            training_data = await self._prepare_training_data()

            if not training_data:
                logger.warning("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è")
                return False

            # –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            retrain_results = {}

            for prediction_type in ['price', 'quality', 'query_optimization']:
                type_data = [ep for ep in training_data if ep.prediction_type == prediction_type]

                if len(type_data) >= self.learning_config['min_training_samples']:
                    success = await self._retrain_specific_model(prediction_type, type_data)
                    retrain_results[prediction_type] = success
                else:
                    logger.warning(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {prediction_type}: {len(type_data)}")
                    retrain_results[prediction_type] = False

            # –û—Ü–µ–Ω–∫–∞ –æ–±—â–µ–≥–æ —É—Å–ø–µ—Ö–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
            successful_retrains = sum(1 for result in retrain_results.values() if result)
            total_models = len(retrain_results)

            self.system_stats['learning_progress'] = successful_retrains / total_models
            self.system_stats['last_retraining'] = datetime.now().isoformat()

            logger.info(f"‚úÖ –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {successful_retrains}/{total_models} –º–æ–¥–µ–ª–µ–π")

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã
            await self._save_learning_state()

            return successful_retrains > 0

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
            return False

    async def _prepare_training_data(self) -> List[LearningEpisode]:
        """üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –æ—á–∏—Å—Ç–∫–æ–π –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
        try:
            # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —ç–ø–∏–∑–æ–¥—ã
            recent_episodes = list(self.learning_episodes)[-1000:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 1000 —ç–ø–∏–∑–æ–¥–æ–≤

            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            cleaned_episodes = []
            for episode in recent_episodes:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                if episode.actual_result is None:
                    continue

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–∞–∑—É–º–Ω–æ—Å—Ç—å –∑–Ω–∞—á–µ–Ω–∏–π
                if (episode.prediction <= 0 or episode.actual_result <= 0 or
                        episode.error is None or episode.error > episode.actual_result * 10):
                    continue

                cleaned_episodes.append(episode)

            logger.info(f"üìä –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(cleaned_episodes)} —ç–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            return cleaned_episodes

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return []

    async def _retrain_specific_model(self, model_type: str, episodes: List[LearningEpisode]):
        """üéØ –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å ML"""
        try:
            if not episodes:
                return False

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ features –∏ targets
            X, y = [], []
            feature_names = []

            for episode in episodes:
                features = self._extract_learning_features(episode)
                if features and episode.actual_result is not None:
                    X.append(features)
                    y.append(episode.actual_result)

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–º–µ–Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –ø—Ä–æ—Ö–æ–¥–µ
                    if not feature_names:
                        feature_names = list(features.keys())

            if len(X) < self.learning_config['min_training_samples']:
                return False

            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ numpy arrays
            X_array = np.array([list(x.values()) for x in X])
            y_array = np.array(y)

            # –í—ã–±–æ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –º–æ–¥–µ–ª–∏
            if model_type == 'price':
                success = await self._train_regression_model(X_array, y_array, model_type)
            elif model_type == 'quality':
                success = await self._train_classification_model(X_array, y_array, model_type)
            else:
                success = await self._train_optimization_model(X_array, y_array, model_type)

            if success:
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –º–æ–¥–µ–ª–∏
                model_key = f"{model_type}_v1"
                if model_key in self.model_versions:
                    self.model_versions[model_key].training_samples = len(X)
                    self.model_versions[model_key].last_used = datetime.now().isoformat()

                    # –†–∞—Å—á–µ—Ç –Ω–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
                    metrics = await self._calculate_model_metrics(X_array, y_array, model_type)
                    self.model_versions[model_key].metrics.update(metrics)

                logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {model_type} —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∞ –Ω–∞ {len(X)} –ø—Ä–∏–º–µ—Ä–∞—Ö")

            return success

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ {model_type}: {e}")
            return False

    async def _train_regression_model(self, X: np.ndarray, y: np.ndarray, model_type: str) -> bool:
        """üìà –û–±—É—á–µ–Ω–∏–µ —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            from sklearn.ensemble import RandomForestRegressor
            from sklearn.model_selection import train_test_split
            from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/validation
            X_train, X_val, y_train, y_val = train_test_split(
                X, y, test_size=self.learning_config['validation_split'], random_state=42
            )

            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            model = RandomForestRegressor(
                n_estimators=50,
                max_depth=15,
                random_state=42,
                min_samples_split=5
            )

            model.fit(X_train, y_train)

            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            y_pred = model.predict(X_val)
            mae = mean_absolute_error(y_val, y_pred)
            mse = mean_squared_error(y_val, y_pred)
            r2 = r2_score(y_val, y_pred)

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ—á–Ω–∞
            if r2 > self.learning_config['performance_threshold'] - 0.2:  # –ë–æ–ª–µ–µ –º—è–≥–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
                # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ joblib
                # joblib.dump(model, f'{model_type}_model.joblib')

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                self.feature_importance[model_type] = dict(zip(
                    [f"feature_{i}" for i in range(X.shape[1])],
                    model.feature_importances_
                ))

                return True
            else:
                logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {model_type} –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ—á–Ω–∞: R¬≤={r2:.3f}")
                return False

        except ImportError:
            logger.warning("‚ö†Ô∏è scikit-learn –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ")
            return await self._train_simple_model(X, y, model_type)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏: {e}")
            return False

    async def _train_classification_model(self, X: np.ndarray, y: np.ndarray, model_type: str) -> bool:
        """üéØ –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            from sklearn.ensemble import RandomForestClassifier
            from sklearn.model_selection import train_test_split
            from sklearn.metrics import accuracy_score, precision_score, recall_score

            # –ë–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            y_binary = (y > np.median(y)).astype(int)

            X_train, X_val, y_train, y_val = train_test_split(
                X, y_binary, test_size=self.learning_config['validation_split'], random_state=42
            )

            model = RandomForestClassifier(
                n_estimators=50,
                max_depth=15,
                random_state=42
            )

            model.fit(X_train, y_train)

            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            y_pred = model.predict(X_val)
            accuracy = accuracy_score(y_val, y_pred)
            precision = precision_score(y_val, y_pred, zero_division=0)
            recall = recall_score(y_val, y_pred, zero_division=0)

            if accuracy > self.learning_config['performance_threshold']:
                # joblib.dump(model, f'{model_type}_model.joblib')
                return True
            else:
                logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {model_type} –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ—á–Ω–∞: accuracy={accuracy:.3f}")
                return False

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")
            return await self._train_simple_model(X, y, model_type)

    async def _train_optimization_model(self, X: np.ndarray, y: np.ndarray, model_type: str) -> bool:
        """‚ö° –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
        return await self._train_simple_model(X, y, model_type)

    async def _train_simple_model(self, X: np.ndarray, y: np.ndarray, model_type: str) -> bool:
        """üîÑ –£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∫–∞–∫ fallback"""
        try:
            # –ü—Ä–æ—Å—Ç–∞—è –ª–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –±–µ–∑ scikit-learn
            if len(X) < 2:
                return False

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
            X_normalized = (X - np.mean(X, axis=0)) / (np.std(X, axis=0) + 1e-8)

            # –ü—Ä–æ—Å—Ç–∞—è –ª–∏–Ω–µ–π–Ω–∞—è –º–æ–¥–µ–ª—å
            coefficients = np.linalg.lstsq(X_normalized, y, rcond=None)[0]

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –∫–∞–∫ –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å
            self.adaptation_rules[f"{model_type}_coefficients"] = coefficients.tolist()

            logger.info(f"üîÑ –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å {model_type} –æ–±—É—á–µ–Ω–∞")
            return True

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è: {e}")
            return False

    def _extract_learning_features(self, episode: LearningEpisode) -> Dict[str, float]:
        """üîç –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏–∑ —ç–ø–∏–∑–æ–¥–∞"""
        try:
            features = {}

            # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ —Ñ–∏—á —ç–ø–∏–∑–æ–¥–∞
            episode_features = episode.features
            if isinstance(episode_features, dict):
                for key, value in episode_features.items():
                    if isinstance(value, (int, float)):
                        features[f"episode_{key}"] = float(value)
                    elif isinstance(value, bool):
                        features[f"episode_{key}"] = 1.0 if value else 0.0

            # –ü—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            context = episode.context
            if isinstance(context, dict):
                for key, value in context.items():
                    if isinstance(value, (int, float)):
                        features[f"context_{key}"] = float(value)

            # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            try:
                episode_time = datetime.fromisoformat(episode.timestamp.replace('Z', '+00:00'))
                features['hour_of_day'] = episode_time.hour / 24.0
                features['day_of_week'] = episode_time.weekday() / 7.0
                features['is_weekend'] = 1.0 if episode_time.weekday() >= 5 else 0.0
            except:
                features.update({'hour_of_day': 0.5, 'day_of_week': 0.5, 'is_weekend': 0.0})

            # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            features['prediction_confidence'] = episode.confidence
            features['has_actual_result'] = 1.0 if episode.actual_result is not None else 0.0

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            numeric_features = {k: v for k, v in features.items() if isinstance(v, (int, float))}

            return numeric_features

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
            return {}

    async def _calculate_model_metrics(self, X: np.ndarray, y: np.ndarray, model_type: str) -> Dict[str, float]:
        """üìä –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏"""
        try:
            if model_type == 'price':
                # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
                from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
                from sklearn.model_selection import cross_val_score

                # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
                dummy_pred = np.full_like(y, np.mean(y))
                mae_baseline = mean_absolute_error(y, dummy_pred)

                # –û—Ü–µ–Ω–∫–∞ –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)
                current_mae = self.system_stats['avg_error']
                improvement = max(0, (mae_baseline - current_mae) / mae_baseline)

                return {
                    'mae': current_mae,
                    'mse': current_mae ** 2,
                    'r2': improvement,
                    'improvement_over_baseline': improvement
                }

            elif model_type == 'quality':
                # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
                success_rate = self.system_stats['successful_predictions'] / max(1, self.system_stats['total_episodes'])
                return {
                    'accuracy': success_rate,
                    'precision': success_rate,
                    'recall': success_rate,
                    'f1_score': success_rate
                }

            else:
                return {'success_rate': 0.7, 'improvement': 0.1}

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫: {e}")
            return {'error': 1.0}

    async def _analyze_feature_importance(self):
        """üîç –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        try:
            if not self.learning_episodes:
                return

            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ —ç–ø–∏–∑–æ–¥–æ–≤
            all_features = defaultdict(list)
            errors = []

            for episode in self.learning_episodes:
                if episode.actual_result is not None and episode.error is not None:
                    features = self._extract_learning_features(episode)
                    for feature, value in features.items():
                        all_features[feature].append(value)
                    errors.append(episode.error)

            # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
            feature_correlations = {}
            for feature, values in all_features.items():
                if len(values) == len(errors):
                    correlation = np.corrcoef(values, errors)[0, 1]
                    if not np.isnan(correlation):
                        feature_correlations[feature] = abs(correlation)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ø-10 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            top_features = dict(sorted(feature_correlations.items(),
                                       key=lambda x: x[1], reverse=True)[:10])

            self.feature_importance['correlation_analysis'] = top_features
            logger.info(f"üîç –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –≤–∞–∂–Ω–æ—Å—Ç—å {len(top_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")

    async def _adaptive_rule_optimization(self, episode: LearningEpisode):
        """‚ö° –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–∞–≤–∏–ª –Ω–∞ –ª–µ—Ç—É"""
        try:
            # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –æ—à–∏–±–æ–∫
            if episode.error is not None and episode.error > episode.prediction * 0.2:
                # –ë–æ–ª—å—à–∞—è –æ—à–∏–±–∫–∞ - –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                context = episode.context or {}

                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –ø–æ—Ö–æ–∂–∏—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤
                context_key = self._create_context_key(context)
                current_rule = self.adaptation_rules.get(context_key, {'error_count': 0, 'adjustment': 1.0})

                current_rule['error_count'] += 1
                current_rule['last_updated'] = datetime.now().isoformat()

                # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –º–Ω–æ–∂–∏—Ç–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—à–∏–±–∫–∏
                error_ratio = episode.error / episode.prediction
                adjustment = 1.0 / (1.0 + error_ratio * self.learning_config['adaptive_learning_rate'])
                current_rule['adjustment'] *= adjustment

                self.adaptation_rules[context_key] = current_rule

                logger.debug(f"‚ö° –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–æ –ø—Ä–∞–≤–∏–ª–æ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {context_key}")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {e}")

    def _create_context_key(self, context: Dict[str, Any]) -> str:
        """üîë –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª—é—á–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        try:
            # –°–æ–∑–¥–∞–µ–º —Ö–µ—à –∏–∑ –∑–Ω–∞—á–∏–º—ã—Ö –ø–æ–ª–µ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            significant_fields = {
                'category': context.get('category', 'unknown'),
                'has_brand': context.get('has_brand', False),
                'condition': context.get('condition', 'unknown'),
                'seller_rating_tier': 'high' if context.get('seller_rating', 0) > 4.5 else 'low'
            }

            context_str = json.dumps(significant_fields, sort_keys=True)
            return hashlib.md5(context_str.encode()).hexdigest()[:8]

        except:
            return "default"

    async def get_learning_insights(self) -> Dict[str, Any]:
        """üìä –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Å–∞–π—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã –æ–±—É—á–µ–Ω–∏—è"""
        try:
            # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è
            recent_episodes = list(self.learning_episodes)[-100:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 100 —ç–ø–∏–∑–æ–¥–æ–≤
            recent_errors = [e.error for e in recent_episodes if e.error is not None]

            if recent_errors:
                recent_avg_error = np.mean(recent_errors)
                error_trend = "—É–ª—É—á—à–∞–µ—Ç—Å—è" if recent_avg_error < self.system_stats['avg_error'] else "—É—Ö—É–¥—à–∞–µ—Ç—Å—è"
            else:
                recent_avg_error = 0.0
                error_trend = "–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö"

            # –¢–æ–ø –º–æ–¥–µ–ª–µ–π –ø–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
            model_performance = []
            for model_name, model_data in self.model_versions.items():
                if 'r2' in model_data.metrics:
                    score = model_data.metrics['r2']
                elif 'accuracy' in model_data.metrics:
                    score = model_data.metrics['accuracy']
                else:
                    score = 0.5

                model_performance.append({
                    'model': model_name,
                    'score': score,
                    'samples': model_data.training_samples
                })

            model_performance.sort(key=lambda x: x['score'], reverse=True)

            return {
                'system_stats': self.system_stats,
                'recent_performance': {
                    'avg_error': recent_avg_error,
                    'trend': error_trend,
                    'success_rate': self.system_stats['successful_predictions'] / max(1, self.system_stats[
                        'total_episodes'])
                },
                'top_models': model_performance[:3],
                'feature_insights': self._get_feature_insights(),
                'adaptation_rules_count': len(self.adaptation_rules),
                'learning_progress': f"{self.system_stats['learning_progress']:.1%}",
                'recommendations': await self._generate_learning_recommendations()
            }

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Å–∞–π—Ç–æ–≤: {e}")
            return {'error': str(e)}

    def _get_feature_insights(self) -> List[Dict[str, Any]]:
        """üîç –ò–Ω—Å–∞–π—Ç—ã –æ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        insights = []

        for model_type, importance_dict in self.feature_importance.items():
            if importance_dict:
                top_feature = max(importance_dict.items(), key=lambda x: x[1])
                insights.append({
                    'model_type': model_type,
                    'most_important_feature': top_feature[0],
                    'importance_score': top_feature[1],
                    'total_features_analyzed': len(importance_dict)
                })

        return insights

    async def _generate_learning_recommendations(self) -> List[str]:
        """üí° –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –æ–±—É—á–µ–Ω–∏—è"""
        recommendations = []

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        if self.system_stats['total_episodes'] < 100:
            recommendations.append("–ù–∞–∫–æ–ø–∏—Ç–µ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π (–º–∏–Ω–∏–º—É–º 100 —ç–ø–∏–∑–æ–¥–æ–≤)")

        if self.system_stats['successful_predictions'] / max(1, self.system_stats['total_episodes']) < 0.6:
            recommendations.append("–£–≤–µ–ª–∏—á—å—Ç–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")

        if len(self.adaptation_rules) < 5:
            recommendations.append("–†–∞—Å—à–∏—Ä—å—Ç–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ª—É—á—à–µ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫ —Ä–∞–∑–Ω—ã–º —Å—Ü–µ–Ω–∞—Ä–∏—è–º")

        if not recommendations:
            recommendations.append("–°–∏—Å—Ç–µ–º–∞ –æ–±—É—á–∞–µ—Ç—Å—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ. –ü—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö")

        return recommendations

    async def _save_learning_state(self):
        """üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã –æ–±—É—á–µ–Ω–∏—è"""
        try:
            state = {
                'system_stats': self.system_stats,
                'model_versions': {k: asdict(v) for k, v in self.model_versions.items()},
                'feature_importance': self.feature_importance,
                'adaptation_rules': self.adaptation_rules,
                'last_saved': datetime.now().isoformat()
            }

            with open('learning_system_state.json', 'w', encoding='utf-8') as f:
                json.dump(state, f, indent=2, ensure_ascii=False)

            logger.info("üíæ –°–æ—Å—Ç–æ—è–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ: {e}")

    async def load_learning_state(self):
        """üì• –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã –æ–±—É—á–µ–Ω–∏—è"""
        try:
            with open('learning_system_state.json', 'r', encoding='utf-8') as f:
                state = json.load(f)

            self.system_stats = state['system_stats']

            for model_name, model_data in state['model_versions'].items():
                self.model_versions[model_name] = ModelPerformance(**model_data)

            self.feature_importance = state['feature_importance']
            self.adaptation_rules = state['adaptation_rules']

            logger.info("üì• –°–æ—Å—Ç–æ—è–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –æ–±—É—á–µ–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–æ")

        except FileNotFoundError:
            logger.info("üì• –§–∞–π–ª —Å–æ—Å—Ç–æ—è–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —á–∏—Å—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è: {e}")

    def get_detailed_stats(self) -> Dict[str, Any]:
        """üìà –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã"""
        return {
            'learning_episodes_count': len(self.learning_episodes),
            'system_stats': self.system_stats,
            'model_performance': {
                name: asdict(performance) for name, performance in self.model_versions.items()
            },
            'feature_importance_summary': {
                model: dict(sorted(imp.items(), key=lambda x: x[1], reverse=True)[:3])
                for model, imp in self.feature_importance.items()
            },
            'adaptation_rules_summary': {
                'total_rules': len(self.adaptation_rules),
                'recently_updated': len([
                    rule for rule in self.adaptation_rules.values()
                    if datetime.fromisoformat(rule['last_updated']) > datetime.now() - timedelta(days=1)
                ])
            }
        }


# üî• –ê–ª–∏–∞—Å –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
LearningSystem = LearningSystem